---
title: "General thoughts"
number-sections: true
---

## Motivation

The purpose of a statistical analysis plan (SAP) - which will often also contain a (partial) data management plan - is to improve the quality of studies. The SAP will often be based on (or an appendix to) a protocol where the overall outline of a study is presented with fewer technical details.

### The research question - what is the estimand?

Writing a detailed SAP forces the researcher/study group to think clearly about what the aim of the study is (which estimand[^1] needs to be estimated?), and how to achieve this aim (which estimator can provide the best estimate?). Possibly, the group will come to the conclusion that a reliable estimate cannot be made with the data at hand, in which case they can abandon the study and save themselves (along with reviewers, editors, taxpayers, and other innocent bystanders) a lot of time and resources which can then be spent on something fruitful (possibly collecting the data needed).

[^1]: There are possibly several estimands.

![[xkcd.com/1838](https://xkcd.com/1838/)](./figs/machine_learning.png){#fig-ml}

Speaking from experience, it is not uncommon that a researcher wants an answer to a question that is so vague that several different analyses could be carried out, and all be said to provide a relevant answer to the overall question.

A question like "What is the occurrence of dementia in individuals with chronic kidney disease (CKD)?" can be a good overall question, but there is no unique answer to that because the question is not specific. It could be interpreted in several ways:

1.  Among people with CKD living in Denmark today, how large a proportion also have dementia?
    -   This question could be answered using a cross sectional design.
2.  Among people who lived with CKD in Denmark 10 years ago how large a proportion have had dementia since?
    -   This question could be answered with a cohort design, using appropriate time-to-event methods to take censoring and the competing risk of death into account.
3.  Among people with incident CKD in the period 2010-2025 without prevalent dementia at the time of CKD, how large a proportion have developed dementia since?
    -   Again, a cohort design with time-to-event methods could be used to answer this question, but notice that it will be a different cohort compared to the above.

Even these questions are not completely clear. The first question interprets "occurrence" as "prevalence", while the second and third aim to provide estimates of an incompletely defined "risk", seeing that risk strictly speaking only makes sense if a time frame is also specified, e.g., 10-year risk.

If the question is not clear before the answer is sought, there is a significant risk of p-hacking[^2] or HARKing.[^3]

[^2]: [xkcd on p-hacking](https://xkcd.com/882/)

[^3]: [Wikipedia on HARKing](https://en.wikipedia.org/wiki/HARKing).

### Definitions and data

Once it is clear what the specific research question is, i.e., what the estimand is, it is also relevant to consider how the population and the individual variables are defined.

Continuing with the example of CKD and dementia, there will be several ways to identify these conditions from registries. Therefore, even if a SAP has been written in great detail, i.e., considerations on how to handle missing data are made, estimation methods described, table shells ready to be populated etc., it is still important to also describe how a population with "incident CKD", say, can be identified from registries. Likewise, it needs to be specified how "dementia" and any other variable necessary for the analyses should be defined.

Defining populations and variables is data management and not statistical analyses per se. That does not make specification of these aspects less important, this is just to point out that a *data management plan* is also essential in most registry-based studies.

### Two documents or one?

The data management plan and the SAP can often be written as one coherent document with no explicit distinction between the two parts. However, it can still be relevant to keep in mind that data management and statistical analyses are in principle separate parts/phases of a study. In international/multicenter studies, it is generally advisable to use a common data model, so that analytic scripts (scripts needed to carry out statistical analyses as specified in the SAP) can be shared, ensuring the same methods are applied at all centers. To facilitate this, each center must provide a dataset that complies with certain rules, i.e., variables must have specific names, types, formats, etc. as specified by the coordinating center.

However, data management will generally have to differ between centers at some initial stage. At the most low-practical level, registries and their variables will have different names and structures. There can also be qualitative differences, e.g., primary vs. secondary care data, granularity of diagnosis/procedure/... codes, or precision of time variables,[^4] all of which may require different approaches between centers. The point being, that in multicenter studies each site probably needs to have its own data management plan, whereas statistical analyses should follow common scripts aligning to a shared SAP.

[^4]: Examples from Danish registries include laboratory data which hold dates and time stamps (hours and minutes), whereas the National Health Insurance Service Registry only includes week number.

In the following, it will be assumed that the data management plan is incorporated into the SAP.

## Elements of the SAP

This section lists elements you might include in a SAP. Not everything will be relevant for all projects, so do not feel obliged to include something about each of these topics.

Writing a SAP should be a collaborative effort involving all those who are expected to have their name on the final paper. That does not mean everyone should contribute equally, people should chip in where their skills and competencies are relevant.

While reporting guidelines such as STROBE, TRIPOD or TARGET do not provide guidance as to how studies should be conducted, it will still be a good idea to have these reporting guidelines in mind when writing the SAP.

### Log of changes

Near the start of the SAP there should be a log of changes, documenting any changes made after the data management or analyses are started. The log should contain dates where changes are made, what the changes are, and reasons for the changes. This will both serve as a reminder within the group of why decisions were made, and if the statistician is replaced the new statistician will be able to get a quick overview of the history of the project (and find reasons for deviations between what is actually done, and what was specified in the protocol).[^5] Whenever a SAP is revised/updated it should be saved as a new document with a version number (e.g., SAPv1, SAPv2, ...).

[^5]: In @tbl-changelog an entry is made for the completion of the first version of the SAP for timeline purposes. This is not necessary and can be omitted.

```{r, echo = FALSE, eval = TRUE}
#| label: tbl-changelog
#| tbl-cap: |

library(gt)

df <- data.frame(
  Date = c("20/06/2023",
           "08/12/2023",
           "31/03/2024"),
  Change = c("NA (First version)",
             "HRs estimated by Cox regression replaced by RRs computed using strata-specific risks estimated by the Aalen-Johansen estimator",
             "Sensitivity analysis added where CKD is defined by eGFR persistently below 60 mL/min/1.73m\U00B2 for at least 90 days"),
  Reason = c("NA",
             "Non-proportional hazards",
             "Reviewer 2 was critical of our initial definition of CKD. We maintain our original definition for primary analyses.")
)

gt(df) |>
  tab_header(title = "Log of changes") |>
  cols_align(align = "left", columns = everything()) |>
  cols_width(
    Date ~ pct(14),
    Change ~ pct(43),
    Reason ~ pct(43)
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = list(
      cells_column_labels(),
      cells_title()
    )
  )

```

### Background and aims / objectives

A brief description of why the study is important, what the aims or objectives are, and which study design is applied. Include any pre-specified hypotheses. It should be clear if the aim is causal, descriptive or predictive in nature.

Consider who the reader is. If this is for a statistician to implement, 2 pages of biology, chemistry and/or anatomy are not helpful. The purpose is not to copy everything from the protocol (if one exists), but to provide the necessary context for the analyses.

### Expected sample size, exposure- and outcome occurrence

In observational (registry-based) studies, sample size/power calculations are irrelevant as they are beyond the control of the researcher. However, it is still relevant to consider how large a population and how many outcomes can be expected. The statistician might not realize before looking at the data, that the population (or an exposure group if relevant) should be counted in tens rather that tens of thousands, in which case all the fancy methods that have been planned might not be possible. If the population is expected to be small (whatever that means), this should be clear to the ones planning the analyses, so they can tell[^6] you what is feasible and what is not.

[^6]: The alternative is that they will yell at/about you for wasting their time.

### Data sources

A list of registries and other data sources to be used, including (links to) detailed data documentation as relevant. Make sure no data source is included in the coding table without being specified here. If you use abbreviations for data sources, make sure to specify these abbreviations here.

### Source population

Consider specifying what the source population is, i.e., population to which the conclusions from the study should apply.

### Study population and index date

Specify the study-, recruitment-, and other periods as relevant. I.e., calendar periods covering various phases of the study. Once specified you should refer to them throughout the document rather than repeating the dates. E.g., if the recruitment period is 1 January 2015 to 31 December 2024, you should "include individuals with condition X in the recruitment period" rather than "include individuals with condition X between 1 January 2015 and 31 December 2024". Avoiding repetitions is helpful when updating/revising the SAP both during drafting and after the first version is completed, because it saves you from having to look for all instances of the elements that need to be changed. If there are many repetitions you might miss some, making the SAP internally inconsistent.

Specify step-by-step how the study population should be derived from the raw registry data. This will often require defining an index date from raw data (make sure it is clear what the index date is), and then a series of in- and exclusion criteria to be applied on this index date.[^7] The variables needed for inclusion/exclusion criteria should be clearly outlined in the coding table.

[^7]: Sometimes there will be more than one day that is important. E.g., for a population with post-surgical infection, it might be necessary to apply some criteria on the date of the surgery and others on the date of the infection.

Prepare a figure shell for a flowchart, and make sure the order of the criteria in the figure aligns with the order the criteria are listed in the text. Sometimes, it can be relevant to leave out some steps used to go from raw data to an index date from the flowchart.[^8] However, all exclusion criteria that are applied subsequent to defining the index date should be included in the flowchart.

[^8]: Consider a project where you want to include all infections of a certain type, e.g., pneumonia. A person might be admitted to a hospital and transferred between departments, and so several records of the same pneumonia may be created in the registries. Another person might have pneumonia several times, and each distinct infection should be included. How you get from raw data, to a dataset where each case of pneumonia is included once is essential in the data management plan but it might not be helpful to the audience for the finished paper.

### Variables

All variables should be listed in a coding table and therefore specific diagnosis codes etc. should not be included here. However, some variables might require elaboration which can be provided here, possibly in subsections reflecting the role of the relevant variables. Complex variables might require a diagram, step-by-step algorithm, or similar to make it very clear how they are defined.

Below are lists with examples of elements to consider, these lists are not exhaustive.

#### Exposure (or baseline variable of primary interest)

Depending of the nature of the study (is the aim causal, descriptive, or predictive) there may or may not be one central baseline variable. Even if there is, in principle it might not be correct to refer to it as an exposure. However, for simplicity the term exposure is used here.

-   Specification of the exposure in intention to treat- vs per protocol-type analyses or in dose-response analyses.
-   Duration of prescriptions and grace period used to combine prescription data into on-treatment periods. (Arguably this can be considered as defining the outcome in a drug utilization study.)
-   Number of prescriptions needed to be exposed, if more than one.
-   If the exposure is categorical, i.e., you make comparisons across classes, you might specify what the reference level should be if this is not obvious (e.g., in comparative effectiveness studies it will generally be less obvious than in comparative safety studies).
-   If the exposure is continuous, i.e., you make comparisons across a scale, you might want to specify what the reference value should be.
-   Categorization of a continuous exposure into classes for analyses where continuous data cannot be applied, e.g., applying Kaplan-Meier or Aalen-Johansen estimators.

#### Follow-up and outcomes

-   Details on when to start and stop follow-up. Make it clear if events on the index date are included or not. E.g.,
    -   "For each primary and secondary event individuals will be followed from the index date (i.e., events on the index date are considered outcomes) until the first of: event of interest, death, emigration, end of study period or 5 years. In per protocol analyses, follow-up is also ended at switching and stopping (defined in the exposure subsection)."
-   Primary and secondary outcomes, e.g., "The primary outcome is MACE, the three secondary outcomes are the individual parts of MACE: AMI, HF, and cardiovascular death."
-   Censoring and competing events, e.g., "Death (of non-cardiovascular causes for MACE and cardiovascular death; of any cause for AMI and HF) is the only competing event; emigration, end of study period and 5 years of follow-up are administrative (non-informative) censoring events. In per protocol analyses switching and stopping are considered informative censoring events."
-   Considerations about/justification for composite outcomes, if relevant.
-   Units of time to be used. Typically, these are based on date-variables and as such days is the finest possible granularity:
    -   If possible, use only days and at most one other unit, e.g., years, and specify how many days there are in this unit.
    -   If you need to include both months and years as units of time, consider if there should be exactly 12 months in a year, e.g., 1 month = 30 days and 1 year = 360 days.
        -   Beware of problems that might arise when using both months and years in the same study. If 1 month = 30 days and 1 year = 365 days, and you decide to censor all follow-up at 60 months (= 1800 days) you cannot get a risk estimate at 5 years (= 1825 days).

#### Covariates

-   If you use laboratory data, and a biomarker is measured in different units, you should specify here which unit to use, along with details on conversion of units.
-   Specify how to handle implausible values (e.g., extreme BMI), truncated values from laboratory data (e.g., biomarker reported as "\<30"), or how to handle biomarkers reported at the limit of the measurable interval (e.g., renin = 1.8 or 550 IU/L) as relevant.
-   If there are several observations of a variable, e.g., HbA1c, during the baseline/lookback period, how should they be combined? E.g., use the most recent, the mean or the median.
-   If a variable is derived from several others (e.g., CHA2DS2-VASc or TNM-coding) you may specify how to get from the separate variables to the combined variable.

#### Study diagram

A study diagram as suggested by [Schneeweiss et al](https://doi.org/10.7326/M18-3079) can be helpful in summarizing the design of the study and the role played by distinct variables. Note that a study diagram is generally repetitious by nature, as it typically will not be sufficiently detailed to stand alone. Make sure that the study diagram aligns with the text and coding table and is updated during revisions. Therefore, for simple study designs, a study diagram might be more of a nuisance than a help in the SAP - at least during the drafting phase. It can still be relevant to include a study diagram in the supplementary material to the final manuscript. For highly complex designs, or in situations where a diagram with heavy annotation is necessary, the templates from Schneeweiss et al might not be sufficient, in which case you may have to design your own diagrams.

### Statistical analyses

This section will often have several subsections depending on the type of study, but it should provide details for the methods required for all analyses with reference to previously specified variables, along with table and figure shells (listed in subsequent sections). All analyses should be represented in either a table or figure shell and vice versa.

#### Baseline characteristics

Describe how to report on baseline characteristics (typically with reference to a table shell). This subsection can often be brief, simply specifying if continuous variables should be reported by their median with interquartile interval or with mean and standard deviations; if one or both levels of dichotomous variables should be reported, e.g., both the number with and without a specific condition; how missingness should be reported, e.g., including the proportion with missing data in the table or in a footnote, and if proportions within a categorical level should be among those with non-missing data or among the entire population. (E.g., smoking: classified as smoker, nonsmoker, missing; if 24% have missing smoking status, 38% are classified as smokers and 38% as nonsmokers, should the proportion of smoking be 38% / 76% = 50% or should it be 38%?)

#### Outcome analyses

If there is an outcome in the study, specify how it should be analyzed:

-   Types of regression models used (if any). This includes outcome models, as well as models to estimate propensity scores, disease risk scores or other models in two-step procedures.
-   How should variables be included in regression models, e.g., continuous variables can be included linearly, as splines, or using some other transformation. Which interactions should be included, if any.
-   Which non-parametric methods to use, if applicable, e.g., Kaplan-Meier, Aalen-Johansen, or mean cumulative count.
-   How to assess uncertainty in estimates if the statistical software does not provide an unbiased one by default, e.g., robust sandwich methods or bootstrapping could be used - specify the number of bootstrap samples, and how to derive point estimates and confidence intervals based on the sampling distribution obtained from bootstrapping.
-   How to assess testable assumptions (like proportional hazards), and what to do if they are not sufficiently met.
-   If there will be missing data, briefly mention how it will be handled with elaboration in a separate section.
-   A brief mention of sensitivity analyses can be considered but will generally need elaboration in a separate subsection.
-   Consider specifying the software and packages to use. (This can be important in multicenter studies.)
-   Consider specifying a seed[^9] to be applied when randomness is used in analyses (e.g., for bootstrap sampling, for sampling of controls in case-control analyses, or for multiple imputation) for exact replication of results.

[^9]: It will generally be over the top to specify which seed to use, simply specifying that a seed should be used will often be sufficient. However, if the data are small, the choice of seed can potentially impact the results qualitatively in which case it can be helpful to specify a seed in advance. Do note, however, that [seeds need to be arbitrary](https://blog.genesmindsmachines.com/p/if-your-random-seed-is-42-i-will)

#### Missing data

If missing data is an issue, specify

-   How much missingness is expected for variables that are expected to be incomplete. (This will help assess which methods are appropriate to use.)
-   How to handle missingness:
    -   Complete case analyses.
    -   Multiple imputation; specify how to derive the number of imputations and how to do the imputations.
    -   Single random imputation; specify how to do the imputation.
    -   Single (mean) value imputation.
    -   Full-flexed Bayesian methods.
If missing data is known not to be an issue you can specify why. E.g. absence of recording of diagnoses in the registry will be regarded as absence of the condition and not be seen as missing data.

#### Subgroup analyses {#sec-subgroup}

The terminology across fields is not completely consistent. Here, "subgroup analyses" refers to analyses where the population is split into subgroups so that each observation from the overall group belongs to exactly one subgroup. The aim of subgroup analyses will generally be to assess if the effect or association observed in the overall group differs across levels of the subgroups, e.g., if there is effect measure modification/treatment effect heterogeneity. Analyses aiming at assessing robustness with regards to potential residual confounding should be listed under @sec-sensitivityAnalyses.

Specify exactly which analyses are to be repeated within subgroups, e.g., is it only the main or also (selected) sensitivity analyses; all or only primary outcomes. List the variables used to define subgroups. If some of these variables are continuous which threshold(s) should be used for categorization. Specify how effect measure modification should be quantified if relevant.

#### Sensitivity analyses {#sec-sensitivityAnalyses}

While the estimands of subgroup analyses, as outlined above, and the primary analysis differ, e.g., treatment effect heterogeneity vs. treatment effect, sensitivity analyses aim at assessing the robustness of the estimate for the estimands in the (main) analyses.

Describe which sensitivity analyses should be performed. All sensitivity analyses should be justifiable, and it is recommended to specify why the individual sensitivity analysis is carried out, e.g., which assumption is challenged by a given sensitivity analysis. Some decisions might be arbitrary but of such minor importance that running a sensitivity analysis will be qualitatively redundant.

Some examples of sensitivity analyses are listed below. Clearly stipulate which sensitivity analyses should be combined. Bear in mind that the number of analyses will grow exponentially with the length of the list if all combinations of sensitivity analyses are conducted.

-   Quantitative bias analyses to address residual confounding, misclassification (of any variable) or selection bias.
-   If some assumptions in the main analyses are questionable, you might rerun the analyses under different assumptions.
-   Repeating analyses where arbitrary choices are varied, e.g., the length of grace period, the use of outpatient data and/or secondary diagnoses for outcomes, etc.
-   Varying censoring mechanisms and competing risks. E.g., prophylactic mastectomy and oophorectomy are essentially competing events to breast and ovarian cancers, however, it is possible though unlikely to develop cancer in residual tissue. Similar considerations can be made for amputations of limbs or live organ donations - do such procedures change the risk of the outcome to an extent where they are considered competing events; should individuals be censored; or should they still be followed beyond this procedure?
-   Complete case analyses if the main analysis relies on imputation, or vice versa.
-   If the confounding structure is assumed to vary across calendar time, analyzing calendar periods separately might be relevant.[^10]
-   Similar considerations can be made across other variables. E.g., if the confounding structure is assumed to differ across sexes regression models might be misspecified for the overall population if insufficient interaction terms are included, therefore the analyses could be repeated within each sex. Note that the aim of this analysis would not be the same as the aim of a subgroup analysis by sex as outlined in @sec-subgroup. One is about residual confounding the other about effect measure modification.
-   Repeating analyses within restricted populations.[^11]
-   Negative control analysis if there is a negative exposure or negative outcome available.

[^10]: Note that even though this could be mistaken as a subgroup analysis it is listed under sensitivity analyses. Effect measure modification does not make sense to do across calendar time, because no future individual will belong the any of the subgroups covered by a specific time period. Only if calendar time can be considered a proxy of an unmeasured variable that can be seen for future patients will treatment effect heterogeneity across calendar time periods make sense in a subgroup analysis. NB Seasons are conceptually different from calendar time.

[^11]: If a known confounder, e.g., smoking, is hard to measure in the data, and a proxy for smoking exists so that those with this proxy can be assumed to smoke, but those without cannot be assumed not to smoke (there is a high positive predictive value but a low negative predictive value), smoking status cannot be imputed in those without the proxy, and restriction to the subpopulation of smokers may provide a less biased (but also less generalizable and less precise) estimate than in the full population. Depending on the strength and prevalence of the confounder it may be preferable to conduct the main analysis within the restricted rather than the broader population.

### Intermediate reporting of results within the study group {#sec-sanity}

Specify in advance if there are intermediate results, e.g., population size, baseline characteristics, etc., to be shared within the research group before further analyses are carried out. The purpose would be to prevent knowledge about the results of the primary analyses from influencing which or how analyses are conducted. These intermediate steps should be seen as quality control and an attempt to avoid p-hacking. E.g., if it is obvious from table 1 that a variable is incorrectly captured, then that variable can be revised before including it in any further analyses. In studies using propensity score weighting or matching for balancing baseline variables, it will often be natural to assess the achieved balance before running outcome analyses.

It may be preferable not to have this as a separate section but instead to integrate these intermediate results across the SAP where they appear naturally when running the analyses.

### Compliance with rules from data providers

This section is relevant in all projects, but of particular importance in collaborations with external partners with limited knowledge of registry-based research conducted at Statistics Denmark or the Health Data Authorities.

Describe how to ensure compliance with the rules specified by data providers, e.g., Statistics Denmark. E.g.:

-   Numbers less than 5 will be masked, other counts will be reported accurately. (Preferable when only a small number of output tables are required.)
-   All counts will be rounded to nearest 10 - rates and percentages will be computed based on rounded numbers. (Perhaps preferable when many output tables are required, e.g., because there are many different outcomes. Not feasible in small study population where $n=15$ and $n=24$ might be too different to both be reported as $n=20$.).
-   Percentiles of a continuous variable will be reported with a number of decimals ensuring at least 5 individuals can be expected to have this value.

### Quality control

This section should describe the procedures implemented to ensure the accuracy and reliability of the statistical programming and results. Low-level quality control includes checking the full distribution of all (continuous) variables, high-level quality control includes having separate individuals implement the SAP independently and comparing results.

### Table shells

Make a set of tables that are empty but otherwise ready to be published. There should a table shell for each table to be included in the manuscript or supplementary materials of the publication. Furthermore, if there are additional numbers to be used in the manuscript ("data not shown"), make table shells for these numbers as well.

### Figure shells

As for table shells, there should be a specification of each figure to be included in the publication. Remember to make a figure shell for the flow chart!

It is generally harder to make shells for figures than for tables, so consider copying/linking to figures from other publications that can be used as templates. Specify axis-labels, colors etc. if they need to be specific.[^12]

[^12]: It will often be quick to change these parameters, but if you know in advance what they should be you might as well spell it out explicitly.

### Coding table {#sec-codingTable}

Provide all codes (e.g., diagnosis/procedure/ATC/SNOMED/NPU/etc.) needed to define the study population, exposure, outcome, and covariates, in a structured manner. This structure can include subsections within the table outlining the different roles different variables have (in-/exclusion criteria, exposure etc.) as in @tbl-codingtable. The coding table can be a separate file (e.g., a spreadsheet) or part of the SAP.

State whether subcodes should be included by default or not. E.g., assume you want to include a variable for breast cancer and a record of DC50 or any of its subcodes are sufficient to define breast cancer, then:

-   If subcodes are included by default then it will be sufficient to specify DC50 in the coding table.
-   If subcodes are not included by default,
    -   you can specify DC50 and a note that for this variable/code subcodes should be included, or
    -   you can specify the complete list of codes starting with DC50 (at time of writing): DC50 DC500 DC500A DC500B DC501 DC502 DC503 DC504 DC505 DC506 DC508 DC509.
        -   Note that subcodes might change over time, i.e., if you do not have a list with all historic subcodes you might miss records.

Make it easy to copy/import codes from the coding table to analytic scripts:

-   Avoid unnecessary characters that need to be deleted, like "\*" in "DK70\*" or "." in "DK70.0".
-   Be consistent
    -   for ICD10-codes either always include the leading D or never include it,
    -   use the same separator between codes in lists: "I20, I21, I22" or "I20 I21 I22".
-   To the extent possible avoid intervals: "DC77-DC79" is hardly faster to write than "DC77 DC78 DC79", however, "DC00-DC43" is sensible.
-   Make sure you distinguish between "O" and "0".

Strive for brevity:

-   In "DK70 DK700 DK701 DK702 DK703 DK704 DK709" the first code is a parent code of the others, so the list is equivalent to "DK70" when subcodes are included.
-   Sort the codes and remove duplicates within variables.
    -   Historically statisticians have received SAPs where a code is included more than once for the same variable - presumably because codes are copied from various different previous projects with little or no thought about overlaps. If codes from three previous projects are reused (copied and pasted) to define condition C in the current project, and "DJ25" is copied from all three previous projects, and it is decided that it should not be used after all, it needs to be deleted 3 times. There is a large chance this will not happen if the list of codes for C is long and unstructured, in which case the statistician is still going to use "DJ25" to define C.

The coding table should have a row for each variable used in the study. In many projects some variables will require data from different registries in which case the cells in the row need to be split (e.g., diabetes can be defined from hospital, prescription, and laboratory data requiring the diabetes row has three subrows).

The columns in the coding table could include but are not limited to:

-   Variable names reflecting how they are included in table and/or figure shells and the SAP, e.g., "Glucose lowering drugs".
-   Data source from which the codes are extracted, e.g., the Danish National Patient Registry/DNPR.
-   The actual codes to be used, e.g., "DJ25".
-   Patient- and diagnosis type (applicable to Danish National Patient Registry); in-, out-, ER-patients, primary or secondary diagnoses.
-   Lookback from index date for baseline variables or variables used for in- or exclusion (this may vary between variables).
-   Notes on issues that are relevant for a specific variable that does not warrant a column in itself. E.g., thresholds for biomarkers defining conditions like chronic kidney disease (eGFR) or type 2 diabetes mellitus (HbA1c); or that a diagnosis code for mycosis fungoides must be made at a department of dermatology to be included.
-   Supporting variables; which variables from the registries should be used to define the specific study variable? In the coding table example below, "department of infectious diseases" needs to be defined, this can be described in the notes, or it could be defined in a column of supporting variables. Supporting variables could also specify which date variables to use for diagnoses, surgical procedures, and treatments. However, if a general rule on dates (e.g., "d_inddto/dato_start are used for diagnoses, d_odto/dato_start used for surgical procedures and in-hospital treatments") can be applied, "Supporting variables" may be redundant as a column in the coding table as there will essentially be no variation in its information.
-   Usually, explicit variable names to be used when programming, e.g., "gld180" (for "Glucose lowering drugs within 180 days") are unnecessary to specify. However, they are important when using a common data model and all sites need to produce structurally similar datasets for analysis. Make sure the variable names comply with the software package to be used and be mindful that some software packages are case sensitive.

The details on defining variables using data from several recordings or data sources should be described in the appropriate sections elsewhere in the SAP rather than within the coding table.

If a prevalent outcome is an exclusion criterion, you can consider specifying it separately under outcomes and exclusion criteria. This might be relevant if only inpatient records should be used for outcomes, but all records should be used for exclusion. To avoid repetitions, the codes to be used can be specified under exclusion criteria only; under outcomes it can be stated that the codes to be used are the same as for the exclusion criteria (or vice versa).

```{r, echo = FALSE}
#| label: tbl-codingtable
#| tbl-cap: |
#|   Suggested structure for a coding table. Code column intentionally left blank. Make codes easy to transfer to statistical software.

library(gt)

tbl <- tibble::tribble(
                 ~Variable,            ~`Data source`, ~Codes, ~`Patient type`,     ~`Diagnosis types`,  ~Lookback,                                        ~Notes,
                "Exposure",                      "Prescription registry",     "",          "NA",                  "NA",       "NA",                                           "",
                  "SGLT2i", "",     "",            "",                   "",         "",                                            "",
                 "GLP-1RA", "",     "",            "",                   "",         "",      "Exclude brand names Saxenda and Wegovy",
           "In-/exclusion",                      "",     "",            "",                   "",         "",                                            "",
                   "T2DM/Glucose lowering drugs", "Prescription registry",     "",          "NA",                 "NA",   "1 year",                                            "",
              "T2DM/HbA1c",   "Laboratory registry",     "",          "NA",                 "NA",  "3 years",                "Any HbA1c > \u2026 indicates T2DM",
          "T2DM/diagnoses",      "Patient registry",     "",         "All", "Primary, secondary", "10 years",                                            "",
          "Recent plague or Cholera", "Patient registry",     "",          "All",                 "Primary, secondary",   "90 days",                                            "",
              
                       "\u2026",                      "",     "",            "",                   "",         "",                                            "",
                "Outcomes",                      "Patient registry",     "",   "Inpatient",            "Primary",         "", "Only at a department of infectious diseases",
                  "Plague",      "",     "",            "",                   "",         "",                                            "",
                 "Cholera",      "",     "",            "",                   "",         "",                                            "",
           "Comorbidities",                      "",     "",            "",                   "",         "",                                            "",
                       "\u2026",                      "",     "",            "",                   "",         "",                                            "",
            "Comedication",                      "",     "",            "",                   "",         "",                                            "",
                       "\u2026",                      "",     "",            "",                   "",         "",                                            "",
              "Biomarkers",                      "",     "",            "",                   "",         "",                                            "",
                       "\u2026",                      "",     "",            "",                   "",         "",                                            ""
  )




gt(tbl) |>
  tab_header(title = "Coding table") |>
  cols_align(align = "left", columns = everything()) |>
  opt_align_table_header(align = c("left")) |>
  cols_width(
    Variable ~ pct(14),
    `Data source` ~ pct(14),
    Codes ~ pct(16),
    `Patient type` ~ pct(14),
    `Diagnosis types` ~ pct(14),
    Lookback ~ pct(14),
    Notes ~ pct(14)
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = list(
      cells_column_labels(),
      cells_title()
    )
  ) |>
  tab_style_body(
    style = cell_text(style = "italic"),
    columns = c(1),
    pattern = "Exposure|In-|Outcomes|Comor|Comed|Bioma"
  ) |>
  tab_style(
    style = list(cell_fill(color = "#cccccc")),
    locations = cells_body(rows = c(1, 4, 10, 13, 15, 17))
  )

```

### References

List of literature referenced in the SAP

## Pitfalls to avoid

1.  Avoid repetitions.
    -   Whenever possible specify things exactly once, then there is exactly one place to change this when revising. This means that the statistician/statistical programmer does not accidentally read the recruitment period the one place you overlooked when revising.
2.  Do not write codes from the coding table in the text, it serves no purpose when it is also specified in a coding table (see previous point about repetitions).
3.  Avoid circular reasoning/definitions. It can be necessary to refer to a later section of the SAP, but make sure that this later section does not point back, so that in effect, the index date is defined as the date of the index date, or similar.

## Simple tip for specifying the order of inclusion-/exclusion criteria

Draw timelines of hypothetical patients' records in registries. How might records of various conditions, relative to each other and the recruitment period, affect who gets in- and excluded? Who would you want to include and who not? How can you set up the order of in- and exclusion criteria to obtain the desired study population? Of particular importance, is the point in your inclusion-/exclusion criteria ordering where you define the index date. Once the index date is set, it is generally of minor importance how subsequent criteria are ordered from the point of view of the data manager.

Consider this description of a study population from a hypothetical protocol:

> We will include individuals diagnosed with herpes zoster between 2000 and 2020 using the Danish National Patient Registry. All patients are required to have been diagnosed at a department of dematology to be included. Patient will be included at the time of their first diagnosis.

Beyond having a diagnosis of herpes zoster, the protocol outlines three key elements to consider when settling on the order of the inclusion-/exclusion criteria:

-   calendar time of diagnosis,
-   department of dermatology, and
-   first observation per patient.

These criteria can be ordered in 6 different ways, and the above ordering is based on the ordering they are mentioned in the text, but that might not be optimal.

To get a feeling of who you want to include, and how you achieve that, you might consider different hypothetical diagnosis patterns for herpes zoster as in @fig-timelines. Each line represents a hypothetical patient.

```{r, echo = FALSE, eval = TRUE, warning=FALSE}
#| label: fig-timelines
#| fig-cap: |
#|   Hypothetical herpes zoster diagnosis patterns

library(ggplot2)

patients <- data.frame(
  id = paste("Patient", factor(1:6)),
  start = rep(1994, 6),
  stop = rep(2023, 6)
)

visits <- data.frame(
  start = c(
    1998, 2005, 2017,
    2007, 2010,
    2002, 2008,
    1995, 2001,
    2007,
    1999
  ),
  id = paste(
    "Patient", factor(
      c(
        1, 1, 1,
        2, 2,
        3, 3,
        4, 4,
        5,
        6
      )
    )
  ),
  Department = factor(
    c(
      "Other", "Dermatology", "Other",
      "Dermatology", "Other",
      "Other", "Dermatology",
      "Dermatology", "Dermatology",
      "Other",
      "Dermatology"
    )
  )
)

ggplot(patients, aes(x = start, xend = stop, y = id, yend = id, group = id)) +
  geom_vline(xintercept = c(2000,2020), linetype = 2, color = "#cccccc") +
  geom_segment(arrow = arrow(), linewidth = 2) +
  geom_point(
    data = visits,
    aes(color = Department, xend = NULL, yend = NULL),
    size = 6
  ) +

  theme_void(base_size = 15) +
  theme(axis.text = element_text()) +
  scale_x_continuous(breaks = c(2000, 2020)) +
  scale_color_manual(values = c("cyan4", "darkorange")) +
  scale_y_discrete(limits = rev)

```

If we had started by extracting all diagnosis codes from the Danish National Patient Registry and then applied the three criteria in the order given above, we would

1.  ignore the first diagnosis of patients 1, 4 and 6 (they are before the study period - patient 6 is excluded),
2.  ignore the diagnoses given at non-dermatology departments (this removes patient 5), and
3.  include all remaining patients at their first remaining diagnosis at a department of dermatology.

So, for this ordering of inclusion-/exclusion criteria, all patients, who had a diagnosis at a department of dermatology at some point during the study period, would be included regardless of their other diagnoses. That might be reasonable for a descriptive study, e.g.,

-   What characterizes patients with herpes zoster at departments of dermatology in Denmark?

but perhaps less so in a cohort study e.g.,

-   What is the prognosis after incident herpes zoster?

where you only want to include incident cases, so at least patient 4 should also be excluded. Depending on the positive predictive value of diagnoses codes for herpes zoster at non-dermatology departments, patients 1 and 3 can be in- or excluded.

If specificity is prioritized, i.e., you want to increase certainty of the condition being incident, then you probably want to exclude patients with a prevalent diagnosis from non-dermatology departments at the cost of population size, including only patient 2 from @fig-timelines. To do this, you can use the ordering:

1.  restrict to the first observation per patient (this excludes no patient), the time of the diagnosis marks the index date,
2.  exclude patients with index dates outside of the study period (this excludes patients 1, 4 and 6),
3.  exclude patients not seen at a dermatology department on the index date (this excludes patients 3 and 5).

Because they come after selection of the index date, the ordering of the latter two steps is irrelevant when it comes to the final study population. However, there might still be a natural ordering of these steps from a clinical point of view. Keep this in mind when specifying the inclusion-/exclusion sequence.

If a more sensitive approach is used, e.g., because you don't trust diagnosis codes from non-dermatology departments, you might argue that including patients 1-3 is reasonable within the scope of the project. This can be done by reordering the criteria as such:

1.  include observations from dermatology departments only (patient 5 is excluded),
2.  restrict to the first diagnosis per patient, this marks the index date,
3.  restrict to index dates within the study period (patients 4 and 6 are excluded).

Note that the patients in @fig-timelines should not be a representative sample from the source population. They should ideally represent all qualitatively different patient trajectories regardless of how frequent these trajectories are seen in the clinic/data.

## External links

[Graphical Depiction of Longitudinal Study Designs in Health Care Databases](https://doi.org/10.7326/M18-3079)

[Visualizations throughout pharmacoepidemiology study planning, implementation, and reporting](https://doi.org/10.1002/pds.5529)
