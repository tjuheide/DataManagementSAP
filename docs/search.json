[
  {
    "objectID": "pe.html",
    "href": "pe.html",
    "title": "Pharmacoepidemiology",
    "section": "",
    "text": "What?\nThe term “pharmacoepidemiology” covers a range of study types, with various aims, some are causal while others are descriptive:\n\nDoes treatment A affects the risk of some outcome compared to either no treatment or an alternative intervention? (Causal aim.)\nHow many start treatment A and how long do they adhere? (Descriptive aim.)\nWhat is the population risk of (known) side effects of treatment A? (Descriptive aim.)\nWhat is the individual risk of (known) side effects of treatment A? (Predictive aim.)",
    "crumbs": [
      "Pharmacoepidemiology"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data management and statistical analysis plans",
    "section": "",
    "text": "Welcome\nThis site contains some thoughts on what to include in a data management and statistical analysis plan when conducting registry-based epidemiological studies. There is an implicit assumption that the setting is Danish health care registries, however, some of the points can likely be generalized to other settings and fields of research.\nThe first version of the site will only feature a single example, however, depending on supply and demand more examples may be added over time.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "General thoughts",
    "section": "",
    "text": "The purpose of a statistical analysis plan (SAP) - which will often also contain a (partial) data management plan - is to improve the quality of studies. The SAP will often be based on (or an appendix to) a protocol where the overall outline of a study is presented with fewer technical details.\n\n\nWriting a detailed SAP forces the researcher/study group to think clearly about what the aim of the study is (which estimand1 needs to be estimated?), and how to achieve this aim (which estimator can provide the best estimate?). Possibly, the group will come to the conclusion that a reliable estimate cannot be made with the data at hand, in which case they can abandon the study and save themselves (along with reviewers, editors, taxpayers, and other innocent bystanders) a lot of time and resources which can then be spent on something fruitful.\n\n\n\n\n\n\nFigure 1: xkcd.com/1838\n\n\n\nSpeaking from experience, it is not uncommon that a researcher wants an answer to a question that is so vague that several different analyses could be carried out, and all be said to provide a relevant answer to the overall question.\nA question like “What is the occurrence of dementia in individuals with chronic kidney disease (CKD)?” can be a good overall question, but there is no unique answer to that because the question is not specific. It could be interpreted in several ways:\n\nAmong people with CKD living in Denmark today, how large a proportion also have dementia?\n\nThis question could be answered using a cross sectional design.\n\nAmong people who lived with CKD in Denmark 10 years ago how large a proportion have had dementia since?\n\nThis question could be answered with a cohort design, using appropriate time-to-event methods to take censoring and the competing risk of death into account.\n\nAmong people with incident CKD in the period 2010-2025 without prevalent dementia at the time of CKD, how large a proportion have developed dementia since?\n\nAgain, a cohort design with time-to-event methods could be used to answer this question, but notice that it will be a different cohort compared to the above.\n\n\nEven these questions are not completely clear. The first question interprets “occurrence” as “prevalence”, while the second and third aim to provide estimates of an incompletely defined “risk”, seeing that risk strictly speaking only makes sense if a time frame is also specified, e.g., 10-year risk.\nIf the question is not clear before the answer is sought, there is a significant risk of p-hacking2 or HARKing.3\n\n\n\nOnce it is clear what the specific research question is, i.e., what the estimand is, it is also relevant to consider how the population and the individual variables are defined.\nContinuing with the example of CKD and dementia, there will be several ways to identify these conditions from registries. Therefore, even if a SAP has been written in great detail, i.e., considerations on how to handle missing data are made, relevant subgroups specified, estimation methods described, table shells ready to be populated etc., it is still important to also describe how a population with “incident CKD”, say, can be identified from registries. Likewise, it needs to be specified how “dementia” and any other variable necessary for the analyses should be defined.\nDefining populations and variables is data management and not statistical analyses per se. That does not make specification of these aspects less important, this is just to point out that a data management plan is also essential.\n\n\n\nThe data management plan and the SAP can often be written as one coherent document with no explicit distinction between the two parts. However, it can still be relevant to keep in mind that data management and statistical analyses are in principle separate parts/phases of a study. In international/multicenter studies, it is generally advisable to use a common data model, so that analytic scripts (scripts needed to carry out statistical analyses as specified in the SAP) can be shared, ensuring the same methods are applied at all centers. To facilitate this, each center must provide a data set that complies with certain rules (specific variable names, types, formats, …) specified by the coordinating center.\nHowever, data management will generally have to differ between centers at some initial level. At the most low-practical level, registries and their variables will have different names and structures. There can also be qualitative differences, e.g., primary vs. secondary care data, granularity of diagnosis/procedure/… codes, or precision of time variables,4 all of which may require different approaches between centers.\nIn the following, it will be assumed that the data management plan is incorporated into the SAP.",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#motivation",
    "href": "basics.html#motivation",
    "title": "General thoughts",
    "section": "",
    "text": "The purpose of a statistical analysis plan (SAP) - which will often also contain a (partial) data management plan - is to improve the quality of studies. The SAP will often be based on (or an appendix to) a protocol where the overall outline of a study is presented with fewer technical details.\n\n\nWriting a detailed SAP forces the researcher/study group to think clearly about what the aim of the study is (which estimand1 needs to be estimated?), and how to achieve this aim (which estimator can provide the best estimate?). Possibly, the group will come to the conclusion that a reliable estimate cannot be made with the data at hand, in which case they can abandon the study and save themselves (along with reviewers, editors, taxpayers, and other innocent bystanders) a lot of time and resources which can then be spent on something fruitful.\n\n\n\n\n\n\nFigure 1: xkcd.com/1838\n\n\n\nSpeaking from experience, it is not uncommon that a researcher wants an answer to a question that is so vague that several different analyses could be carried out, and all be said to provide a relevant answer to the overall question.\nA question like “What is the occurrence of dementia in individuals with chronic kidney disease (CKD)?” can be a good overall question, but there is no unique answer to that because the question is not specific. It could be interpreted in several ways:\n\nAmong people with CKD living in Denmark today, how large a proportion also have dementia?\n\nThis question could be answered using a cross sectional design.\n\nAmong people who lived with CKD in Denmark 10 years ago how large a proportion have had dementia since?\n\nThis question could be answered with a cohort design, using appropriate time-to-event methods to take censoring and the competing risk of death into account.\n\nAmong people with incident CKD in the period 2010-2025 without prevalent dementia at the time of CKD, how large a proportion have developed dementia since?\n\nAgain, a cohort design with time-to-event methods could be used to answer this question, but notice that it will be a different cohort compared to the above.\n\n\nEven these questions are not completely clear. The first question interprets “occurrence” as “prevalence”, while the second and third aim to provide estimates of an incompletely defined “risk”, seeing that risk strictly speaking only makes sense if a time frame is also specified, e.g., 10-year risk.\nIf the question is not clear before the answer is sought, there is a significant risk of p-hacking2 or HARKing.3\n\n\n\nOnce it is clear what the specific research question is, i.e., what the estimand is, it is also relevant to consider how the population and the individual variables are defined.\nContinuing with the example of CKD and dementia, there will be several ways to identify these conditions from registries. Therefore, even if a SAP has been written in great detail, i.e., considerations on how to handle missing data are made, relevant subgroups specified, estimation methods described, table shells ready to be populated etc., it is still important to also describe how a population with “incident CKD”, say, can be identified from registries. Likewise, it needs to be specified how “dementia” and any other variable necessary for the analyses should be defined.\nDefining populations and variables is data management and not statistical analyses per se. That does not make specification of these aspects less important, this is just to point out that a data management plan is also essential.\n\n\n\nThe data management plan and the SAP can often be written as one coherent document with no explicit distinction between the two parts. However, it can still be relevant to keep in mind that data management and statistical analyses are in principle separate parts/phases of a study. In international/multicenter studies, it is generally advisable to use a common data model, so that analytic scripts (scripts needed to carry out statistical analyses as specified in the SAP) can be shared, ensuring the same methods are applied at all centers. To facilitate this, each center must provide a data set that complies with certain rules (specific variable names, types, formats, …) specified by the coordinating center.\nHowever, data management will generally have to differ between centers at some initial level. At the most low-practical level, registries and their variables will have different names and structures. There can also be qualitative differences, e.g., primary vs. secondary care data, granularity of diagnosis/procedure/… codes, or precision of time variables,4 all of which may require different approaches between centers.\nIn the following, it will be assumed that the data management plan is incorporated into the SAP.",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#elements-of-the-sap",
    "href": "basics.html#elements-of-the-sap",
    "title": "General thoughts",
    "section": "2 Elements of the SAP",
    "text": "2 Elements of the SAP\n\n2.1 Log of changes\nNear the start of the SAP there should be a log of changes, documenting any changes made after the data management or analyses are started. The log should contain dates where changes are made, what the changes are, and reasons for the changes. This will both serve as a reminder within the group of why decisions were made, and if the statistician is replaced the new statistician will be able to get a quick overview of the history of the project (and find reasons for deviations between what is actually done, and what was specified in the protocol).5\n\n\n\n\nTable 1\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Log of changes\n    \n    \n    \n      Date\n      Change\n      Reason\n    \n  \n  \n    20/06/2023\nNA (First version)\nNA\n    08/12/2023\nHRs estimated by Cox regression replaced by RRs estimated from Aalen-Johansen estimator\nNon-proportional hazards\n    31/03/2024\nSensitivity analysis added where CKD is defined by eGFR persistently below 60 mL/min/1.73m² for at least 90 days\nReviewer 2 was critical of our initial definition of CKD. We maintain our original definition for primary analyses.\n  \n  \n  \n\n\n\n\n\n\n\n\n\n2.2 Background and aims / objectives\nA brief description of why the study is important, and what the aims or objectives are.\nConsider who the reader is. If this is for a statistician to implement, 2 pages of biology, chemistry and/or anatomy are not helpful. The point is not to copy everything from the protocol (if one exists), but to add the necessary context for the analyses.\n\n\n2.3 Miscellaneous methods\nIt is likely that there are several pieces of information that are relevant to write but are rather matter-of-fact in nature and need no further explanation. These parameters may include:\n\nA list the registries to be used.\nStudy period and recruitment period: if these are specified exactly once, they only need to be changed once in the document if they are revised while running the study.\nTime: how long is a month (e.g., 28, 30 or 30.5 days) and/or a year (e.g., 360, 365 or 365.25 days), if these are relevant units of time. Consider if a year should be the same as 12 months if both units are used in the project.\nExposure groups: If there is an exposure/intervention/… and a control/comparison/reference… group, consider specifying these. E.g., the exposure group is SGLT2i-users and the control group is GLP-1RA-users.\nIf the independent variable of primary interest is continuous, consider specifying the reference value. E.g., comparing risk of dementia for different eGFR-values, with a value of 60 mL/min/1.73m2 as the reference\n\n\n\n2.4 Study population and index date\nList in detail how the study population is derived from the raw registry data. This will often require defining an index date from raw data (make sure it is clear what the index date is), and then a series of in- and exclusion criteria to be applied on this index date.6\nPrepare a figure shell for a flowchart, and make sure the order of the criteria in the figure aligns with the order the criteria are listed in the text. In some situations it can be relevant to leave out some of the steps used to go from raw data to an index date from the flowchart, however, all exclusion criteria that are applied after the index date is defined should be included in the flowchart by default.\n\n\n2.5 Variables\nDepending on the study design, different types of variables are necessary to define. A table of codes to be used for the individual variable should always be supplied. This table can be a separate file or an appendix within the same document. (See also Section 2.12 below.)\nSome variables will likely have different roles, and it might be relevant to have separate paragraphs outlining (as relevant):\n\nhow different levels of the exposure variable will be defined,\nhow the outcome is defined,\n\nif time-to-event-data; list competing and censoring events,\n\nother covariates to be reported.\n\nNote that rather than listing all variables in the text, it can be advantageous to point to a table shell (typically for Table 1 describing baseline characteristics), and state that these are the covariates to be included.\n\n\n2.6 Statistical analyses\nThis is the key element of the statistical part of the SAP.\nSome aspects need little explanation. E.g., populating the table shell for Table 1 with baseline characteristics (in generic studies), does not generally require any particular explanation. It might be relevant to explicitly state that continuous variables (except calendar time) will be reported by their median and interquartile interval (Q1-Q3); that for dichotomous variables only one level (e.g., only the number of individuals with heart failure, not those without) will be reported; that for multilevel categorical variables all levels will be reported.\nThe analyses need to be done should be specified. Things to consider:\n\nwhich type of (regression) model, if any, should be used,\nhow should variables be included into the models (e.g., continuous variables as splines, or using some other transformation?),\nhow should assumptions be assessed, and what should be done if they do not hold,\nwhich non-parametric methods (Kaplan-Meier, Aalen-Johansen, …) should be used,\nhow should missing data be handled (this can be a section on its own),\nhow will uncertainty be estimated if it cannot simply be extracted from standard output,\nbias analyses to handle unmeasured confounding (possibly move this to sensitivity analyses),\nsensitivity- and subgroup analyses should have their own sections, but changes from the main analyses could be mentioned here, e.g., we might not assess assumptions for a sensitivity analysis, if they appear to hold for the primary analysis,\nwhich software packages to use (R, SAS, Stata, …)\n\nTo reduce the risk of p-hacking, it is a good idea to pre-specify interim results to evaluate before moving on. E.g., the size of the population and numbers of exposed might be worth sharing within the group before looking into baseline characteristics. If the numbers are off, something needs to be changed. Likewise, it can be a good idea to produce and share the table with descriptive characteristics before looking into outcome analyses (if relevant), again, to see if something is off before the first version of the main results of the study are known. In this way the risk of rerunning analyses until the desired results are found is reduced.\n\n\n2.7 Missing data\nIf missing data are expected it will generally be a good idea to write something about this in advance. Not just how to handle missing data (which can be outlined under statistical analyses), but rather which variables are expected to contain missing data and to what extent.\n\n\n2.8 Stratified analyses\nSpecify if the analyses should be repeated within strata of some variables. Also specify if the results should be compared across strata, i.e., if effect-heterogeneity is assessed in some formal way, e.g., by estimating a parameter for an interaction term, with eyeballing being the implicit alternative.\n\n\n2.9 Sensitivity analyses including subgroup analyses\nHaving gotten this far, surely, some arbitrary choices will have been made, and the impact of some of these might be relevant to consider in sensitivity analyses. Examples could be\n\nuse of secondary diagnoses for outcome identification,\nlength of follow-up,\nwashout- or grace periods,\nrecruitment period,\nlookback period for variables,\nin- or exclusion of specific patient groups.\n\nIf residual confounding is a concern, restriction to specific subgroups can be used.7 E.g., if obesity, alcohol consumption, smoking or other variables that are hard to measure are important confounders, an analysis restricted to individuals who are known or presumed to be similar with respect to these variables (have a record compatible with obesity, alcoholism, COPD, etc.) may be important to run.8\nSplitting and reanalyzing the population on calendar time can be relevant. Perhaps the demographics of the population or the indication of a treatment changes over time, introducing a shift in the confounding pattern. However, calendar time in itself is generally hard to interpret as an effect modifier. At best it is a proxy for some other variable. Therefore calendar time stratification arguably belongs under sensitivity analyses rather than stratified analyses.\n\n\n2.10 Table shells\nMake a set of tables that are empty but otherwise ready to be published. There should a table shell for each table to be included in the manuscript or supplementary materials of the publication.\n\n\n2.11 Figure shells\nAs for table shells, there should be a specification of each figure to be included in the publication. Remember to make a figure shell for the flow chart!\nIt is generally harder to make shells for figures than for tables, so consider copying/linking to figures from other publications that can be used as templates. Specify axis-labels, colors etc. if they need to be specific.9\n\n\n2.12 Coding table\nProvide all (diagnosis/procedure/ATC/SNOMED/NPU/…) codes needed to define the study population, exposure, outcome, and covariates, in a structured manner. The coding table can be a separate file (e.g., a spreadsheet) or part of the SAP. Make it easy to copy/import codes from the coding table to analytic scripts so they do not have to be transcribed manually.\nColumns to include in the coding table could be:\n\nexplicit variable names10 (typically not necessary but can be relevant when using a common data model across sites),\ninformal variable names,11\ndata source from which the codes are pulled, e.g., the Danish National Patient Registry (DNPR),\nthe actual codes to be used, e.g., DN18,\npatient- and diagnosis type (applicable to Danish National Patient Registry); in-, out-, ER-patients, primary or secondary diagnoses,\nlookback from index date,\nnotes on issues that are relevant for a specific variable that does not warrant a column in it self. E.g., thresholds for biomarkers defining conditions like CKD (eGFR) or type 2 diabetes mellitus (HbA1c); that a diagnosis code mycosis fungoides must be made a department of dermatology to be included\n\nFurther structure can be added as in Table 2 where variables are sorted by their role (exposure, in-/exclusion, outcomes, etc.).\n\n\n\n\nTable 2: Suggested structure for a coding table\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Coding table\n    \n    \n    \n      Variable\n      Data source\n      Codes\n      Patient type\n      Diagnosis types\n      Lookback\n      Notes\n    \n  \n  \n    Exposure\n\n\nNA\nNA\nNA\n\n    SGLT2i\nPrescription registry\n\n\n\n\n\n    GLP-1RA\nPrescription registry\n\n\n\n\nExclude brand names Saxenda and Wegovy\n    In-/exclusion\n\n\n\n\n\n\n    T2DM/Glucose lowering drugs\nPrescription registry\n\nNA\nNA\n1 year\n\n    T2DM/HbA1c\nLaboratory registry\n\nNA\nNA\n3 years\nAny HbA1c &gt; … indicates T2DM\n    T2DM/diagnoses\nPatient registry\n\nAll\nPrimary, secondary\n10 years\n\n    Recent plague or Cholera\nPatient registry\n\nAll\nPrimary, secondary\n90 days\n\n    …\n\n\n\n\n\n\n    Outcomes\n\n\nInpatient\nPrimary\n\nOnly at a department of infectious diseases\n    Plague\nPatient registry\n\n\n\n\n\n    Cholera\nPatient registry\n\n\n\n\n\n    Comorbidities\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Comedication\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Biomarkers\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n2.13 References\nList of literature referenced in the SAP",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#pitfalls-to-avoid",
    "href": "basics.html#pitfalls-to-avoid",
    "title": "General thoughts",
    "section": "3 Pitfalls to avoid",
    "text": "3 Pitfalls to avoid\n\nAvoid repetitions.\n\nWhenever possible specify things exactly once, then there is exactly one place to change this when revising. This means that the statistician/statistical programmer does not accidentally read the recruitment period the one place you overlooked when revising.\n\nDo not write codes from the coding table in the text, it serves no purpose when it is also specified in a coding table (also see point 1.).\nAvoid circular reasoning/definitions. It can be necessary to refer to a later section of the SAP, but beware that this later section does not point back so that, e.g., the index date is defined as the date of the index date.",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#simple-tip-for-specifying-the-order-of-inclusion-exclusion-criteria",
    "href": "basics.html#simple-tip-for-specifying-the-order-of-inclusion-exclusion-criteria",
    "title": "General thoughts",
    "section": "4 Simple tip for specifying the order of inclusion-/exclusion criteria",
    "text": "4 Simple tip for specifying the order of inclusion-/exclusion criteria\nDraw timelines of hypothetical patients’ records in registries. How might records of various conditions relative to each other and the recruitment period affect who gets in- and excluded? Who would you want to include and who not? How can you set up the order of in- and exclusion criteria to achieve this? Of particular importance is at which point in the ordering of your inclusion-/exclusion criteria you define the index date. Once the index date is set, it is generally of minor importance how subsequent criteria are ordered from the point of view of the data manager.\nConsider this description of a study population from a hypothetical protocol:\n\nWe will include individuals diagnosed with herpes zoster between 2000 and 2020 using the Danish National Patient Registry. All patients are required to have been diagnosed at a department of dematology to be included. Patient will be included at the time of their first diagnosis.\n\nBeyond having a diagnosis of herpes zoster, the protocol outlines three key elements to consider, when settling on the order of the inclusion-/exclusion criteria:\n\ncalendar time of diagnosis,\ndepartment of dermatology, and\nfirst observation per patient.\n\nThese criteria can be ordered in 6 different ways, and the above ordering is based on the ordering they are mentioned in the text, but that might not be optimal.\nTo get a feeling of who you want to include and how you achieve that you might consider different hypothetical patients with a diagnosis code for herpes zoster as in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Hypothetical herpes zoster diagnosis patterns\n\n\n\n\n\nIf we had started by extracting all diagnosis codes from the Danish National Patient Registry and then applied the three criteria in the order given above, we would\n\nignore the first diagnosis of patients 1, 4 and 6 (they are before the study period - patient 6 is excluded),\nignore the diagnoses given at non-dermatology departments (this removes patient 5), and\ninclude all remaining patients at their first (remaining) diagnosis at a department of dermatology.\n\nSo, for this ordering of inclusion-/exclusion criteria, all patients, who had a diagnosis at a department of dermatology at some point during the study period, would be included regardless of their other diagnoses. That might be reasonable for a descriptive study, e.g.,\n\nWhat characterizes patients with herpes zoster at departments of dermatology in Denmark?\n\nbut perhaps less so in a cohort study e.g.,\n\nWhat is the prognosis after incident herpes zoster?\n\nwhere you only want to include incident cases, so at least patient 4 should also be excluded. Depending on the positive predictive value of diagnoses codes for herpes zoster at non-dermatology departments, patients 1 and 3 can be in- or excluded.\nIf specificity is prioritized, i.e., you want to increase certainty of the condition being incident, then you probably want to exclude patients with a prevalent diagnosis at a non-dermatology department at the cost of population size, including only patient 2 from Figure 2. To do this, you can use the ordering:\n\nrestrict to the first observation per patient (this excludes no patient), the time of the diagnosis marks the index date,\nexclude patients with index dates outside of the study period (this excludes patients 1, 4 and 6),\nexclude patients not seen at a dermatology department on the index date (this excludes patients 3 and 5).\n\nBecause they come after selection of the index date, the ordering of the latter two steps is irrelevant when it comes to the final study population. However, there might still be a natural ordering of these steps from a clinical point of view. Keep this in mind when specifying the inclusion-/exclusion sequence.\nIf a more sensitive approach is used, e.g., because you don’t trust diagnosis codes from non-dermatology departments, you might argue that including patients 1-3 is reasonable within the scope of the project. This can be done by reordering the criteria as such:\n\ninclude observations from dermatology departments only (patient 5 is excluded),\nrestrict to the first diagnosis per patient, this marks the index date,\nrestrict to index dates within the study period (patients 4 and 6 are excluded).",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#external-links",
    "href": "basics.html#external-links",
    "title": "General thoughts",
    "section": "5 External links",
    "text": "5 External links\nGraphical Depiction of Longitudinal Study Designs in Health Care Databases\nVisualizations throughout pharmacoepidemiology study planning, implementation, and reporting",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#footnotes",
    "href": "basics.html#footnotes",
    "title": "General thoughts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are possibly several estimands.↩︎\nxkcd on p-hacking↩︎\nWikipedia on HARKing.↩︎\nExamples from Danish registries include laboratory data which hold dates and time stamps (hours and minutes), whereas the National Health Insurance Service Registry only includes week number.↩︎\nIn Table 1 an entry is made for the completion of the first version of the SAP for timeline purposes. This is not necessary and can be omitted.↩︎\nSometimes there will be more than one day that is important. E.g., for a population with post-surgical infection, it might be necessary to apply some criteria on the date of the surgery and others on the date of the infection.↩︎\nNote the difference between a subgroup analysis and a stratified analysis.↩︎\nDepending on the strength of the confounder it could be considered if the primary analysis should be within the restricted population, coming at the cost of generalizability This consideration should be made before running the analyses.↩︎\nIt will often be quick to change these parameters, but if you know in advance what they should be you might as well spell it out explicitly.↩︎\nI.e., the name the variable should have in the analytic script, e.g., gld_180d. If formal variable names are used, they have to conform to the statistical software package used.↩︎\nI.e., the name as you would refer to it in text, e.g., ‘glucose lowering drugs within 180 days’.↩︎",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "nuac.html",
    "href": "nuac.html",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "",
    "text": "Here is an example of how a statistical analysis plan (SAP) for study applying a new user, active comparator-design could be set up.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#log-of-changes",
    "href": "nuac.html#log-of-changes",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "1 Log of changes",
    "text": "1 Log of changes\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Log of changes\n    \n    \n    \n      Date\n      Change\n      Reason\n    \n  \n  \n    20/06/2023\nNA (First version)\nNA",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#background",
    "href": "nuac.html#background",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "2 Background",
    "text": "2 Background\nSGLT2-inhibitors have been linked with various infections.1 Whether it affects the risk of plague or Cholera remains unknown.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#aim",
    "href": "nuac.html#aim",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "3 Aim",
    "text": "3 Aim\nTo estimate the 5-year relative risk of plague and cholera after initiation of SGLT2i compared to GLP-1RA, which serves as an active comparator to reduce confounding and is assumed not to affect the risk of either kind of infection.2",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#methods",
    "href": "nuac.html#methods",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "4 Methods",
    "text": "4 Methods\nWe will conduct a new user active comparator study using population-based Danish registry data.\n\n4.1 Data sources\nWe will use data from:\n\nThe Danish National Patient Registry (patient registry)\nThe Danish National Prescription Registry (prescription registry)\nThe Register of Laboratory Results for Research (laboratory registry)\nThe Civil Registration System (CRS)\n\nall of which are well known to the statistical programmer and will not be described or documented further here.3\n\n\n4.2 Study and recruitment period\nThe study period will be 1 January 2016 through 31 December 2024. The recruitment period will be 1 January 2016 through 31 December 2021.\n\n\n4.3 Time\nA month is 30 days, and a year is 360 days.4\n\n\n4.4 Exposure groups\n\nSGLT2i (exposure)\nGLP-1RA (comparator)\n\n\n\n4.5 Study population and index date\nWe will include individuals initiating SGLT2i or GLP-1RA during the study period using the inclusion exclusion steps below. The population is restricted to individuals with type 2 diabetes mellitus (T2DM) because SGLT2i is also indicated for heart failure. We assume that restricting to this specific population of individual with a priori increased risk of infections will reduce confounding. Individuals with heart failure will not be excluded in the primary analysis.\n\nExtract data on the exposures from the prescription registry.\nSelect the first ever date per person, this marks the index date of the individual.\nExclude individuals who, on the index date:\n\ninitiate both drugs\nhave incomplete CRS data (sex or date of birth missing)\nare aged &lt; 18 years\ndo not have T2DM\nhave type 1 diabetes mellitus\nhave had either plague or cholera in the previous 3 months\ndo not live in Denmark.\n\nInclude individuals whose index date lies in the recruitment period. 5\n\nIndividuals will be assigned to exposure groups based on the prescription they received on the index date.\n[Here you might want to include a study diagram like Figure 1.6 However, it is repetitious by nature and as such it may not serve any particular purpose when the design is simple. If the design is more complex, a study diagram can serve as visual aid in explaining it.]\n\n\n\n\n\n\nFigure 1: Study diagram\n\n\n\n\n\n4.6 Follow-up and outcomes\nFor plague and cholera (the events of interest), separately, individuals will be followed from their index date until the first of: event of interest, emigration from Denmark, death, end of study period or 5 years, whichever comes first.\nSee the Outcomes section in Table 4 for definitions of plague and cholera. Death and emigration will be captured in the CRS.\n\n\n4.7 Statistical analyses\nThe flowchart (Figure 2) will be populated.\nThe population will be described as outlined in Table 1. Continuous variables will be reported by their median and interquartile intervals (Q1-Q3), while categorical variables will be reported with numbers and percentages. For dichotomous variables, only one level will be presented (e.g., only the number of females, not males, and only numbers with prevalent heart failure, not numbers without, will be reported), whereas all levels will be presented for variables with more than two levels.\nThe outcomes will be analyzed using time to event methods, applying stabilized inverse probability of treatment (sIPT) weighting and multivariable adjustment to handle confounding. Specifically, sIPT weighted cause-specific Cox regression with confounder adjustment (see below) will be used to estimate hazard ratios (HRs) for each event of interest along with 95% confidence intervals (95% CIs). The results will be reported as outlined in Table 2. Using the Aalen-Johansen estimator and sIPT-weighting, we will plot absolute risks against time, as outlined in Figure 4. No crude/unadjusted outcome analyses will be conducted.\nThe sIPT weights will be estimated using logistic regression including the exposure as the dependent variable and the variables listed in Table 1 will be used as independent variables. Continuous variables will be included as restricted cubic splines with knots placed at the deciles of their distributions (to be reconsidered if insufficient balance is achieved after sIPT weighting). Interaction terms will be included for age and sex; age and heart failure; heart failure and duration of T2DM; and country of origin and age.\nWe will use absolute standardized mean differences (ASMDs) to assess balance of baseline variables before and after sIPT weighting (Figure 3). All sIPT weighted ASMDs must be below 0.1 and the ASMDs for age, sex, country of origin, and heart failure must be less than 0.01 to proceed to the outcome analyses. If this is not achieved, possible alternatives7 to an overall logistic regression model will be discussed and investigated.\nBased on the populated versions of Table 1, Figure 2, and Figure 3, the author group must agree that the study population is reasonable with respect to size and characteristics, and that balance between exposure groups is sufficient, before any outcome analysis is carried out.8\nFor added robustness, sIPT weighting will be combined with multivariable adjustment when applying the cause-specific Cox regression. However, we expect relatively few outcomes, so we will only adjust for sex, age and country of origin, and age will be included as restricted cubic splines with 4 knots placed as suggested by Frank Harrell in Regression modelling strategies.\nThe proportionality assumption will be assessed using Schoenfeld residuals. If proportionality cannot be achieved, the 5-year risk estimates from the Aalen-Johansen estimator for each exposure group will be compared to obtain a 5-year risk ratio.9\nFor stratified analyses and sensitivity analyses restricted to subgroups, we will re-estimate the sIPT weights. We will assess the balance of baseline variables in these subgroup analyses using ASMDs. However, we will not re-evaluate the methods if sufficient balance cannot be achieved. Instead, any imbalance will be reported in the manuscript or supplementary material. Outcome analyses will be conducted regardless of the ASMD-values.\nAll 95% CIs will be estimated using bootstrapping with 500 repetitions.\nThe statistical analyses will be conducted using [SAS/R/Stata] version X.X or higher.\n\n\n4.8 Missing data\nWe expect the level of missingness to be low.\n\nFor comorbidities and comedication, absence of a record within the lookback period is assumed to be indicative of absence of the condition; missingness will be 0% by definition.\nKnown age and sex are required for inclusion; missingness will be 0% by definition.\nWe include biomarkers that are expected to be measured regularly for individuals with T2DM, missingness is expected to be less than 20% for all biomarkers.\n\nWe will apply multiple imputation,10 constructing and analyzing 10 data sets. Results across imputed data sets will be aggregated using Rubin’s rule.\n\n\n4.9 Stratified analyses\nWe will estimate risks and conduct cause specific Cox-regressions stratifying on sex, country of origin and age (&lt;65 years versus 65+ years). These results will be reported as outlined in Figure 5. To assess treatment effect heterogeneity, we will include interaction terms for the exposure and each of these variables in separate cause specific Cox models and report the point estimate and 95% CI.11\n\n\n4.10 Sensitivity analyses\nThese sensitivity analyses are to be applied to the overall analysis only, stratifications will not be repeated within subgroups or in on-treatment analyses. Result will be reported as outlined in Table 3.\nWe will conduct a complete case analysis.\nAs we lack data on BMI, and we assume GLP-1RA is associated with higher levels of BMI, we will repeat the analysis within individuals with an obesity diagnosis.\nWe will restrict the analysis to individuals without a diagnosis of heart failure, to increase the likelihood that the indication for drug initiation was T2DM.\nFinally, we will conduct an on-treatment analysis. For this analysis an individual will be censored on the date they switch from one exposure drug to the other, or if they discontinue the treatment they initiated on the index date (index treatment). Discontinuation will be defined as a period of 120 days with no new prescription of the index treatment.12",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#table-shells",
    "href": "nuac.html#table-shells",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "5 Table shells",
    "text": "5 Table shells\n\n\n\n\nTable 1: Naturally, all variables should be listed in a proper SAP.\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Table shell 1. Baseline characteristics.\n    \n    \n    \n       \n      SGLT2i\n      GLP-1RA\n    \n  \n  \n    N\n\n\n    Age, median (Q1-Q3)\n\n\n    Sex\n\n\n    Calendar year\n\n\n      2016-2017\n\n\n      2018-2019\n\n\n      2020-2021\n\n\n    T2DM characteristics\n\n\n    Duration, years (Q1-Q3)\n\n\n    Glucose lowering drugs\n\n\n    Insulin\n\n\n    Metformin\n\n\n    …\n\n\n    Comorbidities\n\n\n    Heart failure\n\n\n    …\n\n\n    Comedication\n\n\n    …\n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Table shell 2. Risk and HRs at 5 years.\n    \n    \n    \n      Outcome\n      Exposure\n      Events\n      Risk (95% CI)\n      HR (95% CI)\n    \n  \n  \n    Plague\nSGLT2i\n\n\n(ref)\n    \nGLP-1RA\n\n\n\n    Cholera\nSGLT2i\n\n\n(ref)\n    \nGLP-1RA\n\n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nTable 3\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Table shell 3. Sensitivity analyses, risk and HRs at 5 years within subgroups and in the on-treatment analysis.\n    \n    \n    \n      Outcome\n      Sensitivity analysis\n      Exposure\n      Events\n      Risk (95% CI)\n      HR (95% CI)\n    \n  \n  \n    Plague\nComplete case\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWith obesity\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWithout heart failure\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nOn-treatment\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    Cholera\nComplete case\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWith obesity\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWithout heart failure\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nOn-treatment\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#figure-shells",
    "href": "nuac.html#figure-shells",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "6 Figure shells",
    "text": "6 Figure shells\nNB. Figure 3, Figure 4, and Figure 5 are simplified mock-ups of what needs to be made for the study. Do not pay particular attention the colors used here.\n\n6.1 Flowchart\nProvide numbers for the flowchart.\n\n\n\n\n\n\n\n\nFigure 2: Flowchart\n\n\n\n\n\n\n\n6.2 Absolute standardized mean differences\nThe ASMD plot will be included in the supplementary materials. Note that there will be ASMDs for each imputed data set, please include all ASMDs (across imputations) in one figure as seen below. Much less variation between imputations is expected than seen here. Please include and order variables as they appear in Table 1.\n\n\n\n\n\n\n\n\nFigure 3: ASMD plot partial template.\n\n\n\n\n\n\n\n6.3 Risk curves\nThe risk curves should be provided in one figure including 2 panels. Replace “Intervention” and “Control” by “SGLT2i” and “GLP-1RA” in the legend.\n\n\n\n\n\n\nFigure 4: Risk curves template.\n\n\n\n\n\n6.4 Forest plots\nMake one for plague and cholera separately. Add a column to the right, presenting the estimates and 95% CIs for the interaction term for stratified analyses (as text). Replace “Intervention” and “Control” by “SGLT2i” and “GLP-1RA”, and “Risk ratio” by “Hazard ratio” in the column header. Colors TBD depending on target journal. Note the strata shown here do not align with those to be done in this study.\n\n\n\n\n\n\nFigure 5: Forest plot template.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#appendix",
    "href": "nuac.html#appendix",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "7 Appendix",
    "text": "7 Appendix\n\n\n\n\nTable 4: Codes to be used in the study. (Naturally, the table should be populated with codes in a proper SAP.)\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Coding table\n    \n    \n    \n      Variable\n      Data source\n      Codes\n      Patient type\n      Diagnosis types\n      Lookback\n      Notes\n    \n  \n  \n    Exposure\nPrescription registry\n\nNA\nNA\nNA\n\n    SGLT2i\n\n\n\n\n\n\n    GLP-1RA\n\n\n\n\n\nExclude brand names Saxenda and Wegovy\n    In-/exclusion\n\n\n\n\n\n\n    T2DM/Glucose lowering drugs\nPrescription registry\n\nNA\nNA\n1 year\n\n    T2DM/HbA1c\nLaboratory registry\n\nNA\nNA\n3 years\nAny HbA1c &gt; … indicates T2DM\n    T2DM/diagnoses\nPatient registry\n\nAll\nPrimary, secondary\n10 years\n\n    Recent plague or Cholera\nPatient registry\n\nAll\nPrimary, secondary\n90 days\n\n    …\n\n\n\n\n\n\n    Outcomes\nPatient registry\n\nInpatient\nPrimary\n\nOnly at a department of infectious diseases\n    Plague\n\n\n\n\n\n\n    Cholera\n\n\n\n\n\n\n    Comorbidities\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Comedication\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Biomarkers\n\n\n\n\n\n\n    …",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#footnotes",
    "href": "nuac.html#footnotes",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn an actual SAP you should provide references throughout the document as is relevant. In this example no references will be provided.↩︎\nThis is likely a stupid study to conduct in Denmark. That is intentional.↩︎\nA data dictionary or other forms of documentation can be essential if the data sources are unknown to the statistical programmer.↩︎\nThis might be over the top but at some point your statistical programmer will have to decide on how to compute age, length of follow-up etc. if units of time are unspecified. Avoiding “months” will often be a good idea if possible, then a year can be 365 days (or 365.25 days to account for leap years) without having confusion as to how long a month needs to be for a year to equal 12 months.↩︎\nNote how the dates are not written here - they are defined in Section 4.2↩︎\nThe colors and notation in Figure 1 are not the ones typically used. No particular thought has been put into the current design.↩︎\nConsider specifying.↩︎\nTo prevent p-hacking or HARKing, consider deciding a priori on certain intermediate milestones where analyses will be paused, and findings/results so far will be discussed in the group. In this way you can prevent yourself from changing the analyses after seeing the primary results, when it could have been done at an earlier stage.↩︎\nOften this is what you would do anyway when using PS-methods. Cox-regression is simply mentioned to point out that PS-methods and multivariable modelling can be combined.↩︎\nMore details should be provided.↩︎\nOr however you want to do that. If you simply go for eyeballing you might want to rewrite/shorten this section a bit.↩︎\nIn an actual study, you might want to do something about informative censoring, e.g., apply inverse probability of censoring weighting.↩︎",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  }
]