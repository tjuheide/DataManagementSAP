[
  {
    "objectID": "pe.html",
    "href": "pe.html",
    "title": "Pharmacoepidemiology",
    "section": "",
    "text": "What?\nThe term “pharmacoepidemiology” covers a range of study types, with various aims, some are causal while others are descriptive:\n\nDoes treatment A affects the risk of some outcome compared to either no treatment or an alternative intervention? (Causal aim.)\nHow many start treatment A and how long do they adhere? (Descriptive aim.)\nWhat is the population risk of (known) side effects of treatment A? (Descriptive aim.)\nWhat is the individual risk of (known) side effects of treatment A? (Predictive aim.)",
    "crumbs": [
      "Pharmacoepidemiology"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data management and statistical analysis plans",
    "section": "",
    "text": "Welcome\nThis site contains some thoughts on what to include in a data management and statistical analysis plan when conducting registry-based epidemiological studies. There is an implicit assumption that the setting is Danish health care registries, however, some of the points can likely be generalized to other settings and fields of research.\nThe first version of the site will only feature a single example, however, depending on supply and demand more examples may be added over time.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "General thoughts",
    "section": "",
    "text": "The purpose of a statistical analysis plan (SAP) - which will often also contain a (partial) data management plan - is to improve the quality of studies. The SAP will often be based on (or an appendix to) a protocol where the overall outline of a study is presented with fewer technical details.\n\n\nWriting a detailed SAP forces the researcher/study group to think clearly about what the aim of the study is (which estimand1 needs to be estimated?), and how to achieve this aim (which estimator can provide the best estimate?). Possibly, the group will come to the conclusion that a reliable estimate cannot be made with the data at hand, in which case they can abandon the study and save themselves (along with reviewers, editors, taxpayers, and other innocent bystanders) a lot of time and resources which can then be spent on something fruitful (possibly collecting the data needed).\n\n\n\n\n\n\nFigure 1: xkcd.com/1838\n\n\n\nSpeaking from experience, it is not uncommon that a researcher wants an answer to a question that is so vague that several different analyses could be carried out, and all be said to provide a relevant answer to the overall question.\nA question like “What is the occurrence of dementia in individuals with chronic kidney disease (CKD)?” can be a good overall question, but there is no unique answer to that because the question is not specific. It could be interpreted in several ways:\n\nAmong people with CKD living in Denmark today, how large a proportion also have dementia?\n\nThis question could be answered using a cross sectional design.\n\nAmong people who lived with CKD in Denmark 10 years ago how large a proportion have had dementia since?\n\nThis question could be answered with a cohort design, using appropriate time-to-event methods to take censoring and the competing risk of death into account.\n\nAmong people with incident CKD in the period 2010-2025 without prevalent dementia at the time of CKD, how large a proportion have developed dementia since?\n\nAgain, a cohort design with time-to-event methods could be used to answer this question, but notice that it will be a different cohort compared to the above.\n\n\nEven these questions are not completely clear. The first question interprets “occurrence” as “prevalence”, while the second and third aim to provide estimates of an incompletely defined “risk”, seeing that risk strictly speaking only makes sense if a time frame is also specified, e.g., 10-year risk.\nIf the question is not clear before the answer is sought, there is a significant risk of p-hacking2 or HARKing.3\n\n\n\nOnce it is clear what the specific research question is, i.e., what the estimand is, it is also relevant to consider how the population and the individual variables are defined.\nContinuing with the example of CKD and dementia, there will be several ways to identify these conditions from registries. Therefore, even if a SAP has been written in great detail, i.e., considerations on how to handle missing data are made, estimation methods described, table shells ready to be populated etc., it is still important to also describe how a population with “incident CKD”, say, can be identified from registries. Likewise, it needs to be specified how “dementia” and any other variable necessary for the analyses should be defined.\nDefining populations and variables is data management and not statistical analyses per se. That does not make specification of these aspects less important, this is just to point out that a data management plan is also essential in most registry-based studies.\n\n\n\nThe data management plan and the SAP can often be written as one coherent document with no explicit distinction between the two parts. However, it can still be relevant to keep in mind that data management and statistical analyses are in principle separate parts/phases of a study. In international/multicenter studies, it is generally advisable to use a common data model, so that analytic scripts (scripts needed to carry out statistical analyses as specified in the SAP) can be shared, ensuring the same methods are applied at all centers. To facilitate this, each center must provide a dataset that complies with certain rules, i.e., variables must have specific names, types, formats, etc. as specified by the coordinating center.\nHowever, data management will generally have to differ between centers at some initial stage. At the most low-practical level, registries and their variables will have different names and structures. There can also be qualitative differences, e.g., primary vs. secondary care data, granularity of diagnosis/procedure/… codes, or precision of time variables,4 all of which may require different approaches between centers. The point being, that in multicenter studies each site probably needs to have its own data management plan, whereas statistical analyses should follow common scripts aligning to a shared SAP.\nIn the following, it will be assumed that the data management plan is incorporated into the SAP.",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#motivation",
    "href": "basics.html#motivation",
    "title": "General thoughts",
    "section": "",
    "text": "The purpose of a statistical analysis plan (SAP) - which will often also contain a (partial) data management plan - is to improve the quality of studies. The SAP will often be based on (or an appendix to) a protocol where the overall outline of a study is presented with fewer technical details.\n\n\nWriting a detailed SAP forces the researcher/study group to think clearly about what the aim of the study is (which estimand1 needs to be estimated?), and how to achieve this aim (which estimator can provide the best estimate?). Possibly, the group will come to the conclusion that a reliable estimate cannot be made with the data at hand, in which case they can abandon the study and save themselves (along with reviewers, editors, taxpayers, and other innocent bystanders) a lot of time and resources which can then be spent on something fruitful (possibly collecting the data needed).\n\n\n\n\n\n\nFigure 1: xkcd.com/1838\n\n\n\nSpeaking from experience, it is not uncommon that a researcher wants an answer to a question that is so vague that several different analyses could be carried out, and all be said to provide a relevant answer to the overall question.\nA question like “What is the occurrence of dementia in individuals with chronic kidney disease (CKD)?” can be a good overall question, but there is no unique answer to that because the question is not specific. It could be interpreted in several ways:\n\nAmong people with CKD living in Denmark today, how large a proportion also have dementia?\n\nThis question could be answered using a cross sectional design.\n\nAmong people who lived with CKD in Denmark 10 years ago how large a proportion have had dementia since?\n\nThis question could be answered with a cohort design, using appropriate time-to-event methods to take censoring and the competing risk of death into account.\n\nAmong people with incident CKD in the period 2010-2025 without prevalent dementia at the time of CKD, how large a proportion have developed dementia since?\n\nAgain, a cohort design with time-to-event methods could be used to answer this question, but notice that it will be a different cohort compared to the above.\n\n\nEven these questions are not completely clear. The first question interprets “occurrence” as “prevalence”, while the second and third aim to provide estimates of an incompletely defined “risk”, seeing that risk strictly speaking only makes sense if a time frame is also specified, e.g., 10-year risk.\nIf the question is not clear before the answer is sought, there is a significant risk of p-hacking2 or HARKing.3\n\n\n\nOnce it is clear what the specific research question is, i.e., what the estimand is, it is also relevant to consider how the population and the individual variables are defined.\nContinuing with the example of CKD and dementia, there will be several ways to identify these conditions from registries. Therefore, even if a SAP has been written in great detail, i.e., considerations on how to handle missing data are made, estimation methods described, table shells ready to be populated etc., it is still important to also describe how a population with “incident CKD”, say, can be identified from registries. Likewise, it needs to be specified how “dementia” and any other variable necessary for the analyses should be defined.\nDefining populations and variables is data management and not statistical analyses per se. That does not make specification of these aspects less important, this is just to point out that a data management plan is also essential in most registry-based studies.\n\n\n\nThe data management plan and the SAP can often be written as one coherent document with no explicit distinction between the two parts. However, it can still be relevant to keep in mind that data management and statistical analyses are in principle separate parts/phases of a study. In international/multicenter studies, it is generally advisable to use a common data model, so that analytic scripts (scripts needed to carry out statistical analyses as specified in the SAP) can be shared, ensuring the same methods are applied at all centers. To facilitate this, each center must provide a dataset that complies with certain rules, i.e., variables must have specific names, types, formats, etc. as specified by the coordinating center.\nHowever, data management will generally have to differ between centers at some initial stage. At the most low-practical level, registries and their variables will have different names and structures. There can also be qualitative differences, e.g., primary vs. secondary care data, granularity of diagnosis/procedure/… codes, or precision of time variables,4 all of which may require different approaches between centers. The point being, that in multicenter studies each site probably needs to have its own data management plan, whereas statistical analyses should follow common scripts aligning to a shared SAP.\nIn the following, it will be assumed that the data management plan is incorporated into the SAP.",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#elements-of-the-sap",
    "href": "basics.html#elements-of-the-sap",
    "title": "General thoughts",
    "section": "2 Elements of the SAP",
    "text": "2 Elements of the SAP\nThis section lists elements you might include in a SAP. Not everything will be relevant for all projects, so do not feel obliged to include something about each of these topics.\nWriting a SAP should be a collaborative effort involving all those who are expected to have their name on the final paper. That does not mean everyone should contribute equally, people should chip in where their skills and competencies are relevant.\nWhile reporting guidelines such as STROBE, TRIPOD or TARGET do not provide guidance as to how studies should be conducted, it will still be a good idea to have these reporting guidelines in mind when writing the SAP.\n\n2.1 Log of changes\nNear the start of the SAP there should be a log of changes, documenting any changes made after the data management or analyses are started. The log should contain dates where changes are made, what the changes are, and reasons for the changes. This will both serve as a reminder within the group of why decisions were made, and if the statistician is replaced the new statistician will be able to get a quick overview of the history of the project (and find reasons for deviations between what is actually done, and what was specified in the protocol).5 Whenever a SAP is revised/updated it should be saved as a new document with a version number (e.g., SAPv1, SAPv2, …).\n\n\n\n\nTable 1\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Log of changes\n    \n    \n    \n      Date\n      Change\n      Reason\n    \n  \n  \n    20/06/2023\nNA (First version)\nNA\n    08/12/2023\nHRs estimated by Cox regression replaced by RRs computed using strata-specific risks estimated by the Aalen-Johansen estimator\nNon-proportional hazards\n    31/03/2024\nSensitivity analysis added where CKD is defined by eGFR persistently below 60 mL/min/1.73m² for at least 90 days\nReviewer 2 was critical of our initial definition of CKD. We maintain our original definition for primary analyses.\n  \n  \n  \n\n\n\n\n\n\n\n\n\n2.2 Background and aims / objectives\nA brief description of why the study is important, what the aims or objectives are, and which study design is applied. Include any pre-specified hypotheses. It should be clear if the aim is causal, descriptive or predictive in nature.\nConsider who the reader is. If this is for a statistician to implement, 2 pages of biology, chemistry and/or anatomy are not helpful. The purpose is not to copy everything from the protocol (if one exists), but to provide the necessary context for the analyses.\n\n\n2.3 Expected sample size, exposure- and outcome occurrence\nIn observational (registry-based) studies, sample size/power calculations are irrelevant as they are beyond the control of the researcher. However, it is still relevant to consider how large a population and how many outcomes can be expected. The statistician might not realize before looking at the data, that the population (or an exposure group if relevant) should be counted in tens rather that tens of thousands, in which case all the fancy methods that have been planned might not be possible. If the population is expected to be small (whatever that means), this should be clear to the ones planning the analyses, so they can tell6 you what is feasible and what is not.\n\n\n2.4 Data sources\nA list of registries and other data sources to be used, including (links to) detailed data documentation as relevant. Make sure no data source is included in the coding table without being specified here. If you use abbreviations for data sources, make sure to specify these abbreviations here.\n\n\n2.5 Source population\nConsider specifying what the source population is, i.e., population to which the conclusions from the study should apply.\n\n\n2.6 Study population and index date\nSpecify the study-, recruitment-, and other periods as relevant. I.e., calendar periods covering various phases of the study. Once specified you should refer to them throughout the document rather than repeating the dates. E.g., if the recruitment period is 1 January 2015 to 31 December 2024, you should “include individuals with condition X in the recruitment period” rather than “include individuals with condition X between 1 January 2015 and 31 December 2024”. Avoiding repetitions is helpful when updating/revising the SAP both during drafting and after the first version is completed, because it saves you from having to look for all instances of the elements that need to be changed. If there are many repetitions you might miss some, making the SAP internally inconsistent.\nSpecify step-by-step how the study population should be derived from the raw registry data. This will often require defining an index date from raw data (make sure it is clear what the index date is), and then a series of in- and exclusion criteria to be applied on this index date.7 The variables needed for inclusion/exclusion criteria should be clearly outlined in the coding table.\nPrepare a figure shell for a flowchart, and make sure the order of the criteria in the figure aligns with the order the criteria are listed in the text. Sometimes, it can be relevant to leave out some steps used to go from raw data to an index date from the flowchart.8 However, all exclusion criteria that are applied subsequent to defining the index date should be included in the flowchart.\n\n\n2.7 Variables\nAll variables should be listed in a coding table and therefore specific diagnosis codes etc. should not be included here. However, some variables might require elaboration which can be provided here, possibly in subsections reflecting the role of the relevant variables. Complex variables might require a diagram, step-by-step algorithm, or similar to make it very clear how they are defined.\nBelow are lists with examples of elements to consider, these lists are not exhaustive.\n\n2.7.1 Exposure (or baseline variable of primary interest)\nDepending of the nature of the study (is the aim causal, descriptive, or predictive) there may or may not be one central baseline variable. Even if there is, in principle it might not be correct to refer to it as an exposure. However, for simplicity the term exposure is used here.\n\nSpecification of the exposure in intention to treat- vs per protocol-type analyses or in dose-response analyses.\nDuration of prescriptions and grace period used to combine prescription data into on-treatment periods. (Arguably this can be considered as defining the outcome in a drug utilization study.)\nNumber of prescriptions needed to be exposed, if more than one.\nIf the exposure is categorical, i.e., you make comparisons across classes, you might specify what the reference level should be if this is not obvious (e.g., in comparative effectiveness studies it will generally be less obvious than in comparative safety studies).\nIf the exposure is continuous, i.e., you make comparisons across a scale, you might want to specify what the reference value should be.\nCategorization of a continuous exposure into classes for analyses where continuous data cannot be applied, e.g., applying Kaplan-Meier or Aalen-Johansen estimators.\n\n\n\n2.7.2 Follow-up and outcomes\n\nDetails on when to start and stop follow-up. Make it clear if events on the index date are included or not. E.g.,\n\n“For each primary and secondary event individuals will be followed from the index date (i.e., events on the index date are considered outcomes) until the first of: event of interest, death, emigration, end of study period or 5 years. In per protocol analyses, follow-up is also ended at switching and stopping (defined in the exposure subsection).”\n\nPrimary and secondary outcomes, e.g., “The primary outcome is MACE, the three secondary outcomes are the individual parts of MACE: AMI, HF, and cardiovascular death.”\nCensoring and competing events, e.g., “Death (of non-cardiovascular causes for MACE and cardiovascular death; of any cause for AMI and HF) is the only competing event; emigration, end of study period and 5 years of follow-up are administrative (non-informative) censoring events. In per protocol analyses switching and stopping are considered informative censoring events.”\nConsiderations about/justification for composite outcomes, if relevant.\nUnits of time to be used. Typically, these are based on date-variables and as such days is the finest possible granularity:\n\nIf possible, use only days and at most one other unit, e.g., years, and specify how many days there are in this unit.\nIf you need to include both months and years as units of time, consider if there should be exactly 12 months in a year, e.g., 1 month = 30 days and 1 year = 360 days.\n\nBeware of problems that might arise when using both months and years in the same study. If 1 month = 30 days and 1 year = 365 days, and you decide to censor all follow-up at 60 months (= 1800 days) you cannot get a risk estimate at 5 years (= 1825 days).\n\n\n\n\n\n2.7.3 Covariates\n\nIf you use laboratory data, and a biomarker is measured in different units, you should specify here which unit to use, along with details on conversion of units.\nSpecify how to handle implausible values (e.g., extreme BMI), truncated values from laboratory data (e.g., biomarker reported as “&lt;30”), or how to handle biomarkers reported at the limit of the measurable interval (e.g., renin = 1.8 or 550 IU/L) as relevant.\nIf there are several observations of a variable, e.g., HbA1c, during the baseline/lookback period, how should they be combined? E.g., use the most recent, the mean or the median.\nIf a variable is derived from several others (e.g., CHA2DS2-VASc or TNM-coding) you may specify how to get from the separate variables to the combined variable.\n\n\n\n2.7.4 Study diagram\nA study diagram as suggested by Schneeweiss et al can be helpful in summarizing the design of the study and the role played by distinct variables. Note that a study diagram is generally repetitious by nature, as it typically will not be sufficiently detailed to stand alone. Make sure that the study diagram aligns with the text and coding table and is updated during revisions. Therefore, for simple study designs, a study diagram might be more of a nuisance than a help in the SAP - at least during the drafting phase. It can still be relevant to include a study diagram in the supplementary material to the final manuscript. For highly complex designs, or in situations where a diagram with heavy annotation is necessary, the templates from Schneeweiss et al might not be sufficient, in which case you may have to design your own diagrams.\n\n\n\n2.8 Statistical analyses\nThis section will often have several subsections depending on the type of study, but it should provide details for the methods required for all analyses with reference to previously specified variables, along with table and figure shells (listed in subsequent sections). All analyses should be represented in either a table or figure shell and vice versa.\n\n2.8.1 Baseline characteristics\nDescribe how to report on baseline characteristics (typically with reference to a table shell). This subsection can often be brief, simply specifying if continuous variables should be reported by their median with interquartile interval or with mean and standard deviations; if one or both levels of dichotomous variables should be reported, e.g., both the number with and without a specific condition; how missingness should be reported, e.g., including the proportion with missing data in the table or in a footnote, and if proportions within a categorical level should be among those with non-missing data or among the entire population. (E.g., smoking: classified as smoker, nonsmoker, missing; if 24% have missing smoking status, 38% are classified as smokers and 38% as nonsmokers, should the proportion of smoking be 38% / 76% = 50% or should it be 38%?)\n\n\n2.8.2 Outcome analyses\nIf there is an outcome in the study, specify how it should be analyzed:\n\nTypes of regression models used (if any). This includes outcome models, as well as models to estimate propensity scores, disease risk scores or other models in two-step procedures.\nHow should variables be included in regression models, e.g., continuous variables can be included linearly, as splines, or using some other transformation. Which interactions should be included, if any.\nWhich non-parametric methods to use, if applicable, e.g., Kaplan-Meier, Aalen-Johansen, or mean cumulative count.\nHow to assess uncertainty in estimates if the statistical software does not provide an unbiased one by default, e.g., robust sandwich methods or bootstrapping could be used - specify the number of bootstrap samples, and how to derive point estimates and confidence intervals based on the sampling distribution obtained from bootstrapping.\nHow to assess testable assumptions (like proportional hazards), and what to do if they are not sufficiently met.\nIf there will be missing data, briefly mention how it will be handled with elaboration in a separate section.\nA brief mention of sensitivity analyses can be considered but will generally need elaboration in a separate subsection.\nConsider specifying the software and packages to use. (This can be important in multicenter studies.)\nConsider specifying a seed9 to be applied when randomness is used in analyses (e.g., for bootstrap sampling, for sampling of controls in case-control analyses, or for multiple imputation) for exact replication of results.\n\n\n\n2.8.3 Missing data\nIf missing data is an issue, specify\n\nHow much missingness is expected for variables that are expected to be incomplete. (This will help assess which methods are appropriate to use.)\nHow to handle missingness:\n\nComplete case analyses.\nMultiple imputation; specify how to derive the number of imputations and how to do the imputations.\nSingle random imputation; specify how to do the imputation.\nSingle (mean) value imputation.\nFull-flexed Bayesian methods. If missing data is known not to be an issue you can specify why. E.g. absence of recording of diagnoses in the registry will be regarded as absence of the condition and not be seen as missing data.\n\n\n\n\n2.8.4 Subgroup analyses\nThe terminology across fields is not completely consistent. Here, “subgroup analyses” refers to analyses where the population is split into subgroups so that each observation from the overall group belongs to exactly one subgroup. The aim of subgroup analyses will generally be to assess if the effect or association observed in the overall group differs across levels of the subgroups, e.g., if there is effect measure modification/treatment effect heterogeneity. Analyses aiming at assessing robustness with regards to potential residual confounding should be listed under Section 2.8.5.\nSpecify exactly which analyses are to be repeated within subgroups, e.g., is it only the main or also (selected) sensitivity analyses; all or only primary outcomes. List the variables used to define subgroups. If some of these variables are continuous which threshold(s) should be used for categorization. Specify how effect measure modification should be quantified if relevant.\n\n\n2.8.5 Sensitivity analyses\nWhile the estimands of subgroup analyses, as outlined above, and the primary analysis differ, e.g., treatment effect heterogeneity vs. treatment effect, sensitivity analyses aim at assessing the robustness of the estimate for the estimands in the (main) analyses.\nDescribe which sensitivity analyses should be performed. All sensitivity analyses should be justifiable, and it is recommended to specify why the individual sensitivity analysis is carried out, e.g., which assumption is challenged by a given sensitivity analysis. Some decisions might be arbitrary but of such minor importance that running a sensitivity analysis will be qualitatively redundant.\nSome examples of sensitivity analyses are listed below. Clearly stipulate which sensitivity analyses should be combined. Bear in mind that the number of analyses will grow exponentially with the length of the list if all combinations of sensitivity analyses are conducted.\n\nQuantitative bias analyses to address residual confounding, misclassification (of any variable) or selection bias.\nIf some assumptions in the main analyses are questionable, you might rerun the analyses under different assumptions.\nRepeating analyses where arbitrary choices are varied, e.g., the length of grace period, the use of outpatient data and/or secondary diagnoses for outcomes, etc.\nVarying censoring mechanisms and competing risks. E.g., prophylactic mastectomy and oophorectomy are essentially competing events to breast and ovarian cancers, however, it is possible though unlikely to develop cancer in residual tissue. Similar considerations can be made for amputations of limbs or live organ donations - do such procedures change the risk of the outcome to an extent where they are considered competing events; should individuals be censored; or should they still be followed beyond this procedure?\nComplete case analyses if the main analysis relies on imputation, or vice versa.\nIf the confounding structure is assumed to vary across calendar time, analyzing calendar periods separately might be relevant.10\nSimilar considerations can be made across other variables. E.g., if the confounding structure is assumed to differ across sexes regression models might be misspecified for the overall population if insufficient interaction terms are included, therefore the analyses could be repeated within each sex. Note that the aim of this analysis would not be the same as the aim of a subgroup analysis by sex as outlined in Section 2.8.4. One is about residual confounding the other about effect measure modification.\nRepeating analyses within restricted populations.11\nNegative control analysis if there is a negative exposure or negative outcome available.\n\n\n\n\n2.9 Intermediate reporting of results within the study group\nSpecify in advance if there are intermediate results, e.g., population size, baseline characteristics, etc., to be shared within the research group before further analyses are carried out. The purpose would be to prevent knowledge about the results of the primary analyses from influencing which or how analyses are conducted. These intermediate steps should be seen as quality control and an attempt to avoid p-hacking. E.g., if it is obvious from table 1 that a variable is incorrectly captured, then that variable can be revised before including it in any further analyses. In studies using propensity score weighting or matching for balancing baseline variables, it will often be natural to assess the achieved balance before running outcome analyses.\nIt may be preferable not to have this as a separate section but instead to integrate these intermediate results across the SAP where they appear naturally when running the analyses.\n\n\n2.10 Compliance with rules from data providers\nThis section is relevant in all projects, but of particular importance in collaborations with external partners with limited knowledge of registry-based research conducted at Statistics Denmark or the Health Data Authorities.\nDescribe how to ensure compliance with the rules specified by data providers, e.g., Statistics Denmark. E.g.:\n\nNumbers less than 5 will be masked, other counts will be reported accurately. (Preferable when only a small number of output tables are required.)\nAll counts will be rounded to nearest 10 - rates and percentages will be computed based on rounded numbers. (Perhaps preferable when many output tables are required, e.g., because there are many different outcomes. Not feasible in small study population where \\(n=15\\) and \\(n=24\\) might be too different to both be reported as \\(n=20\\).).\nPercentiles of a continuous variable will be reported with a number of decimals ensuring at least 5 individuals can be expected to have this value.\n\n\n\n2.11 Quality control\nThis section should describe the procedures implemented to ensure the accuracy and reliability of the statistical programming and results. Low-level quality control includes checking the full distribution of all (continuous) variables, high-level quality control includes having separate individuals implement the SAP independently and comparing results.\n\n\n2.12 Table shells\nMake a set of tables that are empty but otherwise ready to be published. There should a table shell for each table to be included in the manuscript or supplementary materials of the publication. Furthermore, if there are additional numbers to be used in the manuscript (“data not shown”), make table shells for these numbers as well.\n\n\n2.13 Figure shells\nAs for table shells, there should be a specification of each figure to be included in the publication. Remember to make a figure shell for the flow chart!\nIt is generally harder to make shells for figures than for tables, so consider copying/linking to figures from other publications that can be used as templates. Specify axis-labels, colors etc. if they need to be specific.12\n\n\n2.14 Coding table\nProvide all codes (e.g., diagnosis/procedure/ATC/SNOMED/NPU/etc.) needed to define the study population, exposure, outcome, and covariates, in a structured manner. This structure can include subsections within the table outlining the different roles different variables have (in-/exclusion criteria, exposure etc.) as in Table 2. The coding table can be a separate file (e.g., a spreadsheet) or part of the SAP.\nState whether subcodes should be included by default or not. E.g., assume you want to include a variable for breast cancer and a record of DC50 or any of its subcodes are sufficient to define breast cancer, then:\n\nIf subcodes are included by default then it will be sufficient to specify DC50 in the coding table.\nIf subcodes are not included by default,\n\nyou can specify DC50 and a note that for this variable/code subcodes should be included, or\nyou can specify the complete list of codes starting with DC50 (at time of writing): DC50 DC500 DC500A DC500B DC501 DC502 DC503 DC504 DC505 DC506 DC508 DC509.\n\nNote that subcodes might change over time, i.e., if you do not have a list with all historic subcodes you might miss records.\n\n\n\nMake it easy to copy/import codes from the coding table to analytic scripts:\n\nAvoid unnecessary characters that need to be deleted, like “*” in “DK70*” or “.” in “DK70.0”.\nBe consistent\n\nfor ICD10-codes either always include the leading D or never include it,\nuse the same separator between codes in lists: “I20, I21, I22” or “I20 I21 I22”.\n\nTo the extent possible avoid intervals: “DC77-DC79” is hardly faster to write than “DC77 DC78 DC79”, however, “DC00-DC43” is sensible.\nMake sure you distinguish between “O” and “0”.\n\nStrive for brevity:\n\nIn “DK70 DK700 DK701 DK702 DK703 DK704 DK709” the first code is a parent code of the others, so the list is equivalent to “DK70” when subcodes are included.\nSort the codes and remove duplicates within variables.\n\nHistorically statisticians have received SAPs where a code is included more than once for the same variable - presumably because codes are copied from various different previous projects with little or no thought about overlaps. If codes from three previous projects are reused (copied and pasted) to define condition C in the current project, and “DJ25” is copied from all three previous projects, and it is decided that it should not be used after all, it needs to be deleted 3 times. There is a large chance this will not happen if the list of codes for C is long and unstructured, in which case the statistician is still going to use “DJ25” to define C.\n\n\nThe coding table should have a row for each variable used in the study. In many projects some variables will require data from different registries in which case the cells in the row need to be split (e.g., diabetes can be defined from hospital, prescription, and laboratory data requiring the diabetes row has three subrows).\nThe columns in the coding table could include but are not limited to:\n\nVariable names reflecting how they are included in table and/or figure shells and the SAP, e.g., “Glucose lowering drugs”.\nData source from which the codes are extracted, e.g., the Danish National Patient Registry/DNPR.\nThe actual codes to be used, e.g., “DJ25”.\nPatient- and diagnosis type (applicable to Danish National Patient Registry); in-, out-, ER-patients, primary or secondary diagnoses.\nLookback from index date for baseline variables or variables used for in- or exclusion (this may vary between variables).\nNotes on issues that are relevant for a specific variable that does not warrant a column in itself. E.g., thresholds for biomarkers defining conditions like chronic kidney disease (eGFR) or type 2 diabetes mellitus (HbA1c); or that a diagnosis code for mycosis fungoides must be made at a department of dermatology to be included.\nSupporting variables; which variables from the registries should be used to define the specific study variable? In the coding table example below, “department of infectious diseases” needs to be defined, this can be described in the notes, or it could be defined in a column of supporting variables. Supporting variables could also specify which date variables to use for diagnoses, surgical procedures, and treatments. However, if a general rule on dates (e.g., “d_inddto/dato_start are used for diagnoses, d_odto/dato_start used for surgical procedures and in-hospital treatments”) can be applied, “Supporting variables” may be redundant as a column in the coding table as there will essentially be no variation in its information.\nUsually, explicit variable names to be used when programming, e.g., “gld180” (for “Glucose lowering drugs within 180 days”) are unnecessary to specify. However, they are important when using a common data model and all sites need to produce structurally similar datasets for analysis. Make sure the variable names comply with the software package to be used and be mindful that some software packages are case sensitive.\n\nThe details on defining variables using data from several recordings or data sources should be described in the appropriate sections elsewhere in the SAP rather than within the coding table.\nIf a prevalent outcome is an exclusion criterion, you can consider specifying it separately under outcomes and exclusion criteria. This might be relevant if only inpatient records should be used for outcomes, but all records should be used for exclusion. To avoid repetitions, the codes to be used can be specified under exclusion criteria only; under outcomes it can be stated that the codes to be used are the same as for the exclusion criteria (or vice versa).\n\n\n\n\nTable 2: Suggested structure for a coding table. Code column intentionally left blank. Make codes easy to transfer to statistical software.\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Coding table\n    \n    \n    \n      Variable\n      Data source\n      Codes\n      Patient type\n      Diagnosis types\n      Lookback\n      Notes\n    \n  \n  \n    Exposure\nPrescription registry\n\nNA\nNA\nNA\n\n    SGLT2i\n\n\n\n\n\n\n    GLP-1RA\n\n\n\n\n\nExclude brand names Saxenda and Wegovy\n    In-/exclusion\n\n\n\n\n\n\n    T2DM/Glucose lowering drugs\nPrescription registry\n\nNA\nNA\n1 year\n\n    T2DM/HbA1c\nLaboratory registry\n\nNA\nNA\n3 years\nAny HbA1c &gt; … indicates T2DM\n    T2DM/diagnoses\nPatient registry\n\nAll\nPrimary, secondary\n10 years\n\n    Recent plague or Cholera\nPatient registry\n\nAll\nPrimary, secondary\n90 days\n\n    …\n\n\n\n\n\n\n    Outcomes\nPatient registry\n\nInpatient\nPrimary\n\nOnly at a department of infectious diseases\n    Plague\n\n\n\n\n\n\n    Cholera\n\n\n\n\n\n\n    Comorbidities\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Comedication\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Biomarkers\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n2.15 References\nList of literature referenced in the SAP",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#pitfalls-to-avoid",
    "href": "basics.html#pitfalls-to-avoid",
    "title": "General thoughts",
    "section": "3 Pitfalls to avoid",
    "text": "3 Pitfalls to avoid\n\nAvoid repetitions.\n\nWhenever possible specify things exactly once, then there is exactly one place to change this when revising. This means that the statistician/statistical programmer does not accidentally read the recruitment period the one place you overlooked when revising.\n\nDo not write codes from the coding table in the text, it serves no purpose when it is also specified in a coding table (see previous point about repetitions).\nAvoid circular reasoning/definitions. It can be necessary to refer to a later section of the SAP, but make sure that this later section does not point back, so that in effect, the index date is defined as the date of the index date, or similar.",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#simple-tip-for-specifying-the-order-of-inclusion-exclusion-criteria",
    "href": "basics.html#simple-tip-for-specifying-the-order-of-inclusion-exclusion-criteria",
    "title": "General thoughts",
    "section": "4 Simple tip for specifying the order of inclusion-/exclusion criteria",
    "text": "4 Simple tip for specifying the order of inclusion-/exclusion criteria\nDraw timelines of hypothetical patients’ records in registries. How might records of various conditions, relative to each other and the recruitment period, affect who gets in- and excluded? Who would you want to include and who not? How can you set up the order of in- and exclusion criteria to obtain the desired study population? Of particular importance, is the point in your inclusion-/exclusion criteria ordering where you define the index date. Once the index date is set, it is generally of minor importance how subsequent criteria are ordered from the point of view of the data manager.\nConsider this description of a study population from a hypothetical protocol:\n\nWe will include individuals diagnosed with herpes zoster between 2000 and 2020 using the Danish National Patient Registry. All patients are required to have been diagnosed at a department of dematology to be included. Patient will be included at the time of their first diagnosis.\n\nBeyond having a diagnosis of herpes zoster, the protocol outlines three key elements to consider when settling on the order of the inclusion-/exclusion criteria:\n\ncalendar time of diagnosis,\ndepartment of dermatology, and\nfirst observation per patient.\n\nThese criteria can be ordered in 6 different ways, and the above ordering is based on the ordering they are mentioned in the text, but that might not be optimal.\nTo get a feeling of who you want to include, and how you achieve that, you might consider different hypothetical diagnosis patterns for herpes zoster as in Figure 2. Each line represents a hypothetical patient.\n\n\n\n\n\n\n\n\nFigure 2: Hypothetical herpes zoster diagnosis patterns\n\n\n\n\n\nIf we had started by extracting all diagnosis codes from the Danish National Patient Registry and then applied the three criteria in the order given above, we would\n\nignore the first diagnosis of patients 1, 4 and 6 (they are before the study period - patient 6 is excluded),\nignore the diagnoses given at non-dermatology departments (this removes patient 5), and\ninclude all remaining patients at their first remaining diagnosis at a department of dermatology.\n\nSo, for this ordering of inclusion-/exclusion criteria, all patients, who had a diagnosis at a department of dermatology at some point during the study period, would be included regardless of their other diagnoses. That might be reasonable for a descriptive study, e.g.,\n\nWhat characterizes patients with herpes zoster at departments of dermatology in Denmark?\n\nbut perhaps less so in a cohort study e.g.,\n\nWhat is the prognosis after incident herpes zoster?\n\nwhere you only want to include incident cases, so at least patient 4 should also be excluded. Depending on the positive predictive value of diagnoses codes for herpes zoster at non-dermatology departments, patients 1 and 3 can be in- or excluded.\nIf specificity is prioritized, i.e., you want to increase certainty of the condition being incident, then you probably want to exclude patients with a prevalent diagnosis from non-dermatology departments at the cost of population size, including only patient 2 from Figure 2. To do this, you can use the ordering:\n\nrestrict to the first observation per patient (this excludes no patient), the time of the diagnosis marks the index date,\nexclude patients with index dates outside of the study period (this excludes patients 1, 4 and 6),\nexclude patients not seen at a dermatology department on the index date (this excludes patients 3 and 5).\n\nBecause they come after selection of the index date, the ordering of the latter two steps is irrelevant when it comes to the final study population. However, there might still be a natural ordering of these steps from a clinical point of view. Keep this in mind when specifying the inclusion-/exclusion sequence.\nIf a more sensitive approach is used, e.g., because you don’t trust diagnosis codes from non-dermatology departments, you might argue that including patients 1-3 is reasonable within the scope of the project. This can be done by reordering the criteria as such:\n\ninclude observations from dermatology departments only (patient 5 is excluded),\nrestrict to the first diagnosis per patient, this marks the index date,\nrestrict to index dates within the study period (patients 4 and 6 are excluded).\n\nNote that the patients in Figure 2 should not be a representative sample from the source population. They should ideally represent all qualitatively different patient trajectories regardless of how frequent these trajectories are seen in the clinic/data.",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#external-links",
    "href": "basics.html#external-links",
    "title": "General thoughts",
    "section": "5 External links",
    "text": "5 External links\nGraphical Depiction of Longitudinal Study Designs in Health Care Databases\nVisualizations throughout pharmacoepidemiology study planning, implementation, and reporting",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "basics.html#footnotes",
    "href": "basics.html#footnotes",
    "title": "General thoughts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are possibly several estimands.↩︎\nxkcd on p-hacking↩︎\nWikipedia on HARKing.↩︎\nExamples from Danish registries include laboratory data which hold dates and time stamps (hours and minutes), whereas the National Health Insurance Service Registry only includes week number.↩︎\nIn Table 1 an entry is made for the completion of the first version of the SAP for timeline purposes. This is not necessary and can be omitted.↩︎\nThe alternative is that they will yell at/about you for wasting their time.↩︎\nSometimes there will be more than one day that is important. E.g., for a population with post-surgical infection, it might be necessary to apply some criteria on the date of the surgery and others on the date of the infection.↩︎\nConsider a project where you want to include all infections of a certain type, e.g., pneumonia. A person might be admitted to a hospital and transferred between departments, and so several records of the same pneumonia may be created in the registries. Another person might have pneumonia several times, and each distinct infection should be included. How you get from raw data, to a dataset where each case of pneumonia is included once is essential in the data management plan but it might not be helpful to the audience for the finished paper.↩︎\nIt will generally be over the top to specify which seed to use, simply specifying that a seed should be used will often be sufficient. However, if the data are small, the choice of seed can potentially impact the results qualitatively in which case it can be helpful to specify a seed in advance. Do note, however, that seeds need to be arbitrary↩︎\nNote that even though this could be mistaken as a subgroup analysis it is listed under sensitivity analyses. Effect measure modification does not make sense to do across calendar time, because no future individual will belong the any of the subgroups covered by a specific time period. Only if calendar time can be considered a proxy of an unmeasured variable that can be seen for future patients will treatment effect heterogeneity across calendar time periods make sense in a subgroup analysis. NB Seasons are conceptually different from calendar time.↩︎\nIf a known confounder, e.g., smoking, is hard to measure in the data, and a proxy for smoking exists so that those with this proxy can be assumed to smoke, but those without cannot be assumed not to smoke (there is a high positive predictive value but a low negative predictive value), smoking status cannot be imputed in those without the proxy, and restriction to the subpopulation of smokers may provide a less biased (but also less generalizable and less precise) estimate than in the full population. Depending on the strength and prevalence of the confounder it may be preferable to conduct the main analysis within the restricted rather than the broader population.↩︎\nIt will often be quick to change these parameters, but if you know in advance what they should be you might as well spell it out explicitly.↩︎",
    "crumbs": [
      "Home",
      "General thoughts"
    ]
  },
  {
    "objectID": "CDM.html",
    "href": "CDM.html",
    "title": "Multicenter studies - an aside on common data models",
    "section": "",
    "text": "If you plan to coordinate a study across several centers, a common data model is essential to ensure consistent methods across sites. As the coordinating investigator you have a particular responsibility to ensure that all centers are able to align to the common data model. Below are some aspects that would be relevant to consider for smooth collaboration.\n\n\nIf you plan to write code in R/SAS/STATA, make sure that your collaborators have access to the same software at a sufficiently high (or low) version number.\nIt will be an advantage if they are regular users of the same software so they will be able to understand what is done, but this is not essential. Possibly, they will do data management using some other package, save data in a specified file format, that can be loaded and analysed using your scripts. E.g., if you write analytic scripts in R, collaborating center 1 do data management in SAS, while center 2 use Python, and center 3 use STATA,1 it might be a bother for the centers to provide data in .rds-format. Instead, it can be practical for all to save their data in .csv-format, in which case the analytic script should rely on reading .csv-files rather than some other format. It should not be the responsibility of the individual center, to go through the analytic scripts to change the way data is loaded each time data is loaded. That is your responsibility as the coordinating investigator.\nIf you use R or similar software, where you regularly load packages not part of the base program, you need to be sure that all packages you use are available to your collaborators. Using packages installed from GitHub can be problematic if collaborators are only able to get packages from, e.g., CRAN. What is a standard package to you, might not be to your collaborators.\n\n\n\nAssuming centers work at remote servers, similar to SDS or DST, they will be subject to regulations beyond their control. These regulations can vary from site to site, and are relevant to consider when programming.\n\nFile extensions; make sure you save the output using a file extension that you collaborators are allowed to return to you. Common extensions like, .pdf, .png, .docx, .rtf etc. should generally not be problematic, but extensions that are also regularly used for micro data, like .csv, .rds, .sas7bdat or (yikes!) .xlsx2 may pose a problem.\nFile size; while you will generally be unable to know how large an output file will be when collaborators run your scripts, it will be a good idea to know what their limitations are for file sizes. At the time of writing, SDS allows .pdf-files up to 1 MB and .png-files up to 5 MB, your collaborators might have to adhere to a uniform limit of 2 MB regardless of file extension. If your version of figure 1 is at 4 MB you might have to reconsider the resolution, size or format.\nSensitive data; the minimum count allowed in a cell can vary between sites. If your script handles N &lt; n0 in an automated fashion, n0 should either be the maximum limit across sites (for consistency), or easy for collaborators to change in the scripts. Likewise, percentiles (in particular minimum and maximum), figures with outliers (scatter plots, skewed distributions), risk curves with large jumps, etc. may or may not be an issue at various sites. It will be the responsibility of your collaborators to make sure they adhere to local regulations, but do what you can to help.\n\nStill assuming your collaborators work at a remote server, they will need to forward your scripts to their data provider. This process may not be trivial. Take the time you need to write and validate and double check your scripts before sharing them! You will probably have to make a revision at some point, but it should not be 2 days after sending the first batch of analytic scripts. Possibly, one of the centers is a regular collaborator which you can use for feedback and sparring, or they might have less administrative delay. Consider sharing your code with them first to sort out bugs before sharing with all.\n\n\n\nMake sure your instructions for running the scripts come across clearly. Possibly, there are situations where it will be fine to write instructions as comments in the top of an analytic script, but generally people prefer to read a document in a format that was meant for reading (.docx or .pdf). Do not ask your collaborators to read through several hundred lines of code (in a programming language they might not know) to see which output files they should return to you, and which are for local use (e.g., plots to check assumptions). Instead hand them a document in a format that is pleasant to read with\n\ninstructions on required folder structure and possibly paths to specify in a master script,\nwhich programs to run in which order,\nwhich output files are for their eyes only, and which are necessary for you,\netc.\n\nIn short, you should not rely on your collaborators picking up a handful of comments you have written across several scripts, where the bulk of the text is code that they should not be concerned with.\nWhatever you can do to ease the burden on other centers will improve collaboration. They might not be ready to run your scripts when you get in touch. If they furthermore have to (get data providers to) install packages and check all your code to make local adaptations due to incompatible input or output formats, you will delay the time it takes until they return the results to you. Speaking from experience as a non-coordinating collaborator, receiving scripts that return errors, which could have been avoided had the coordinating investigator specified and adhered to a common data model, severely dampens motivation. If this becomes modus operandi, all collaborators might postpone spending time on the project, hoping someone else will go through and debug the scripts.",
    "crumbs": [
      "Home",
      "Multicenter studies - an aside on common data models"
    ]
  },
  {
    "objectID": "CDM.html#collaborating-with-other-groups",
    "href": "CDM.html#collaborating-with-other-groups",
    "title": "Multicenter studies - an aside on common data models",
    "section": "",
    "text": "If you plan to coordinate a study across several centers, a common data model is essential to ensure consistent methods across sites. As the coordinating investigator you have a particular responsibility to ensure that all centers are able to align to the common data model. Below are some aspects that would be relevant to consider for smooth collaboration.\n\n\nIf you plan to write code in R/SAS/STATA, make sure that your collaborators have access to the same software at a sufficiently high (or low) version number.\nIt will be an advantage if they are regular users of the same software so they will be able to understand what is done, but this is not essential. Possibly, they will do data management using some other package, save data in a specified file format, that can be loaded and analysed using your scripts. E.g., if you write analytic scripts in R, collaborating center 1 do data management in SAS, while center 2 use Python, and center 3 use STATA,1 it might be a bother for the centers to provide data in .rds-format. Instead, it can be practical for all to save their data in .csv-format, in which case the analytic script should rely on reading .csv-files rather than some other format. It should not be the responsibility of the individual center, to go through the analytic scripts to change the way data is loaded each time data is loaded. That is your responsibility as the coordinating investigator.\nIf you use R or similar software, where you regularly load packages not part of the base program, you need to be sure that all packages you use are available to your collaborators. Using packages installed from GitHub can be problematic if collaborators are only able to get packages from, e.g., CRAN. What is a standard package to you, might not be to your collaborators.\n\n\n\nAssuming centers work at remote servers, similar to SDS or DST, they will be subject to regulations beyond their control. These regulations can vary from site to site, and are relevant to consider when programming.\n\nFile extensions; make sure you save the output using a file extension that you collaborators are allowed to return to you. Common extensions like, .pdf, .png, .docx, .rtf etc. should generally not be problematic, but extensions that are also regularly used for micro data, like .csv, .rds, .sas7bdat or (yikes!) .xlsx2 may pose a problem.\nFile size; while you will generally be unable to know how large an output file will be when collaborators run your scripts, it will be a good idea to know what their limitations are for file sizes. At the time of writing, SDS allows .pdf-files up to 1 MB and .png-files up to 5 MB, your collaborators might have to adhere to a uniform limit of 2 MB regardless of file extension. If your version of figure 1 is at 4 MB you might have to reconsider the resolution, size or format.\nSensitive data; the minimum count allowed in a cell can vary between sites. If your script handles N &lt; n0 in an automated fashion, n0 should either be the maximum limit across sites (for consistency), or easy for collaborators to change in the scripts. Likewise, percentiles (in particular minimum and maximum), figures with outliers (scatter plots, skewed distributions), risk curves with large jumps, etc. may or may not be an issue at various sites. It will be the responsibility of your collaborators to make sure they adhere to local regulations, but do what you can to help.\n\nStill assuming your collaborators work at a remote server, they will need to forward your scripts to their data provider. This process may not be trivial. Take the time you need to write and validate and double check your scripts before sharing them! You will probably have to make a revision at some point, but it should not be 2 days after sending the first batch of analytic scripts. Possibly, one of the centers is a regular collaborator which you can use for feedback and sparring, or they might have less administrative delay. Consider sharing your code with them first to sort out bugs before sharing with all.\n\n\n\nMake sure your instructions for running the scripts come across clearly. Possibly, there are situations where it will be fine to write instructions as comments in the top of an analytic script, but generally people prefer to read a document in a format that was meant for reading (.docx or .pdf). Do not ask your collaborators to read through several hundred lines of code (in a programming language they might not know) to see which output files they should return to you, and which are for local use (e.g., plots to check assumptions). Instead hand them a document in a format that is pleasant to read with\n\ninstructions on required folder structure and possibly paths to specify in a master script,\nwhich programs to run in which order,\nwhich output files are for their eyes only, and which are necessary for you,\netc.\n\nIn short, you should not rely on your collaborators picking up a handful of comments you have written across several scripts, where the bulk of the text is code that they should not be concerned with.\nWhatever you can do to ease the burden on other centers will improve collaboration. They might not be ready to run your scripts when you get in touch. If they furthermore have to (get data providers to) install packages and check all your code to make local adaptations due to incompatible input or output formats, you will delay the time it takes until they return the results to you. Speaking from experience as a non-coordinating collaborator, receiving scripts that return errors, which could have been avoided had the coordinating investigator specified and adhered to a common data model, severely dampens motivation. If this becomes modus operandi, all collaborators might postpone spending time on the project, hoping someone else will go through and debug the scripts.",
    "crumbs": [
      "Home",
      "Multicenter studies - an aside on common data models"
    ]
  },
  {
    "objectID": "CDM.html#footnotes",
    "href": "CDM.html#footnotes",
    "title": "Multicenter studies - an aside on common data models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf they use SPSS you should reconsider if they are up to the task. If they use Excel you need to report them to the proper authorities.↩︎\nOuputting tables to spreadsheets is fine, but you should not save data in spreadsheet formats! Excel loves converting things into dates.↩︎",
    "crumbs": [
      "Home",
      "Multicenter studies - an aside on common data models"
    ]
  },
  {
    "objectID": "nuac.html",
    "href": "nuac.html",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "",
    "text": "Here is an example of how a statistical analysis plan (SAP) for study applying a new user, active comparator-design could be set up. Some details are intentionally left out - essentially because I am a lazy scoundrel but more importantly because the methods presented here should not be considered a guide on how to conduct such a study.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#log-of-changes",
    "href": "nuac.html#log-of-changes",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "1 Log of changes",
    "text": "1 Log of changes\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Log of changes\n    \n    \n    \n      Date\n      Change\n      Reason\n    \n  \n  \n    20/06/2023\nNA (First version)\nNA",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#background-and-aim",
    "href": "nuac.html#background-and-aim",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "2 Background and aim",
    "text": "2 Background and aim\nSGLT2-inhibitors have been linked with various infections.1 Whether it affects the risk of plague or cholera remains unknown. We aim to estimate the causal effect of SGLT2i-initiation on the risk of plague and cholera.\nWe will conduct a new user active comparator study to estimate the 5-year relative risk of plague and cholera after initiation of SGLT2i compared to GLP-1RA. GLP-1RAs will serve as an active comparator to reduce confounding and is assumed not to affect the risk of either kind of infection.2\nWe hypothesize that there will be no clinically meaningful difference between the two groups.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#sample-size",
    "href": "nuac.html#sample-size",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "3 Sample size",
    "text": "3 Sample size\nWe expect the sample size to be 30-60,000 with at least 10,000 individuals in each exposure group. We assume there will be approximately 100 events.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#methods",
    "href": "nuac.html#methods",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "4 Methods",
    "text": "4 Methods\nWe will conduct a new user active comparator study using population-based Danish registry data.\n\n4.1 Data sources\nWe will use data from:\n\nThe Danish National Patient Registry (patient registry)\nThe Danish National Prescription Registry (prescription registry)\nThe Register of Laboratory Results for Research (laboratory registry)\nThe Civil Registration System (CRS)\n\nall of which are well known to the statistical programmer and will not be described or documented further here.3\n\n\n4.2 Source population\nThe source population is Danish residents with type 2 diabetes mellitus, eligible to initiate either SGLT2is or GLP-1RAs. SGLT2is are also used in patients with other conditions such as heart failure and chronic kidney disease, however, our results will not be directly applicable to these populations, where GLP-1RA is not indicated per se. However, individuals with heart failure or CKD will not be excluded.\n\n\n4.3 Study population and index date\nThe study period will be 1 January 2016 through 31 December 2024, and the recruitment period will be 1 January 2016 through 31 December 2021.\nWe will include individuals initiating SGLT2i or GLP-1RA during the study period using the inclusion-/exclusion steps below.\n\nExtract all available data on the exposures from the prescription registry.\nSelect the first ever date per person, this marks the index date of the individual.\nExclude individuals who, on the index date:\n\ndo not have T2DM\ninitiate both drugs\nhave type 1 diabetes mellitus\nhave incomplete CRS data (sex or date of birth missing, no data on citizenship or country of origin)\nare aged &lt; 18 years\nhave had either plague or cholera in the previous 3 months\ndo not live in Denmark.\n\nInclude individuals whose index date lies in the recruitment period.4\n\nIndividuals will be assigned to exposure groups based on the prescription they received on the index date.\nAt this point, check that there are at least 10,000 individuals in each exposure group. If this is not the case, we need to make sure that the correct codes have been used.\n[Here you might want to include a study diagram like Figure 1.5 However, it is repetitious by nature and as such it may not serve any particular purpose when the design is simple. If the design is more complex, a study diagram can serve as visual aid in explaining it.]\n\n\n\n\n\n\nFigure 1: Study diagram\n\n\n\n\n\n4.4 Variables\nSee Table 4 for codes used to define different variables.\n\n4.4.1 Exposure\nIn the main analyses we will analyze the data in an observational analogue to an intention-to-treat (ITT) analysis.\nIn a sensitivity analysis, we will run an analogue of a per-protocol (PP) analysis. For this analysis we will use the variable \\(\\text{volapk}\\) from the prescription registry as a measure of how many days the individual prescription covers. I.e., the date a prescription is assumed to be empty is \\(\\text{volapk}\\) days after the prescription is filled. We will use a grace period of 90 days to allow for gaps between prescriptions. I.e., someone filling just one prescription will be assumed to be exposed from the date of the prescription until \\(\\text{volapk} + [\\text{length of grace period]}\\) days after the date of the prescription. If a new prescription is filled during the period covered by the previous one, we will consider the two prescriptions as one continuous treatment episode, and set the time the combined episode ends as the date the latter of the two prescriptions is estimated to end.6\n\n\n4.4.2 Follow-up and outcome\nWe will use both years and months as units of follow-up time. A year will be 360 days and a month will be 30 days to ensure that 1 year is 12 months.\nFollow-up will start on the index date, i.e., events observed on the index date will be counted as outcomes.\nIn ITT analyses, for each outcome of interest (plague and cholera separately) follow-up will end on the first of: event of interest, emigration from Denmark, death, end of study period or 5 years, whichever comes first. Death is the only competing risk in this study. Emigration and end of study period are assumed to be non-informative censoring events.\nIn PP analyses follow-up is as for ITT analyses, with addition of these informative censoring events: switch to (filling a prescription of) the other exposure group and treatment cessation (see Section 4.4.1). Handling of informative censoring will be done by inverse probability of censoring weighting as described in Section 4.5.2.2.\nPlague and cholera are considered as co-primary outcomes, there is no secondary outcome.\n\n\n4.4.3 Covariates\nComorbidities and comedication will be considered binary variables (with no missingness by definition) defined by presence of one or more codes in registries.\nCountry of origin will be based on data from the Civil Registration System, and divided into Nordic (reference), other European country, Africa, Asia, other.7\nBiomarkers include HbA1c, eGFR, and uACR, the median value during the lookback period will be used. Missing values will be imputed (see Section 4.5).\nHbA1c will be reported in [here a unit should be specified], conversion from [alternative unit] is done using the formula [formula].\neGFR will be calculated from creatinine using the CKD-EPI[yy] formula [the formula should be inserted]. Creatinine measurements taken during acute illness will be excluded. Acute illness is defined as dates on which the individual was an inpatient or visited the emergency room.8\nuACR will only be included from directly reported values. I.e., we will not seek to compute uACR based on albumin and creatinine values.\nFor all biomarkers we will only use observations with an actual value in the registries. E.g., values reported like “&lt;10” or “&gt;550” will not be included.9\n\n\n\n4.5 Statistical analyses\n\n4.5.1 Descriptive analyses\nThe flowchart (Figure 2) will be populated.\nThe population will be described as outlined in Table 1. Continuous variables will be reported by their median and interquartile intervals (Q1-Q3), while categorical variables will be reported with counts and percentages. For dichotomous variables, only one level will be presented (e.g., only the number of females, not males, and only numbers with prevalent heart failure, not numbers without, will be reported), whereas all levels will be presented for variables with more than two levels. The proportion of missing data will be reported in the table for each biomarker (the only variables for which data can be missing).\n\n\n4.5.2 Outcome analyses\nFor the analyses described below all confidence intervals (CIs) will be estimated using bootstrapping with 500 repetitions, and all weights will be truncated at 10 to reduce variability from overfitted weight models. The confidence interval limits will be determined by percentiles of the sampling distribution from the bootstrap analyses.10 When constructing bootstrap samples we will use the seed 1486187913 for reproducibility.11\n\n4.5.2.1 ITT analyses\nFor ITT analyses, the outcomes will be analyzed using time to event methods, applying stabilized inverse probability of treatment (sIPT) weighting and multivariable adjustment to handle confounding. Specifically, sIPT weighted cause-specific Cox regression with confounder adjustment (see below) will be used to estimate hazard ratios (HRs) for each event of interest, along with 95% CIs. The results will be reported as outlined in Table 2. Using the Aalen-Johansen estimator and sIPT-weighting, we will plot absolute risks against time, as outlined in Figure 4. No crude/unadjusted outcome analyses will be conducted.\nThe sIPT weights will be estimated using logistic regression including the exposure as the dependent variable, and the variables listed in Table 1 as independent variables. Continuous variables will be included as restricted cubic splines with knots placed at the deciles of their distributions (to be reconsidered if insufficient balance is achieved after sIPT weighting). Interaction terms will be included for age and sex; age and heart failure; heart failure and duration of T2DM; age and CKD; CKD and duration of T2DM; and country of origin and age.\nWe will use absolute standardized mean differences (ASMDs) to assess balance of baseline variables before and after sIPT weighting (Figure 3). All sIPT weighted ASMDs must be below 0.1 and the ASMDs for age, sex, country of origin, heart failure, and CKD must be less than 0.01 to proceed to the outcome analyses. If this is not achieved, possible alternatives12 to an overall logistic regression model will be discussed and investigated.\nBased on the populated versions of Table 1, Figure 2, and Figure 3, the entire author group must agree that the study population is reasonable with respect to size and characteristics, and that balance between exposure groups is sufficient, before any outcome analysis is carried out.13\nFor added robustness, sIPT weighting will be combined with multivariable adjustment when applying the cause-specific Cox regression. However, we expect relatively few outcomes, so we will only adjust for sex, age (included as a linear term) and country of origin.\nThe proportionality assumption will be assessed using Schoenfeld residuals. If proportionality cannot be achieved, the 5-year risk estimates from the Aalen-Johansen estimator for each exposure group will be compared to obtain a 5-year risk ratio.14\n\n\n4.5.2.2 PP analyses15\nThe PP analyses will be conducted similarly; the same baseline sIPT weights will be used and the same type of outcome analyses will be conducted but with time-varying weights: To handle informative censoring, inverse probability of censoring (IPC) weighting will be applied. I.e., follow-up will be coarsened into months so that anything happening during the first month of follow-up will be considered to happen at time 1, anything happening in the second month will be considered to happen at time 2 etc. (Events on the index date happen at time 0.) For each individual in the study population, all variables will be updated for each month of follow-up (using a last observation carried forward approach if no new observation is seen for biomarkers, comorbidities can go from absent to present but not the other way, comedication can go from absent to present and from present to absent if it is not seen within the lookback period at a given time during follow-up). The IPC weights will be estimated within each exposure group, using a pooled logistic regression (running one model across all months of follow-up) using the time-updated variables. The model will include the same variables as the baseline model used to estimate sIPT weights and a variable for month of follow-up which will be included as a spline with 4 knots placed as suggested by Frank Harrell in Regression Model Strategies. Based on the model, the month-specific probability of adhering16 to the protocol, \\(p_a\\), will be computed so that the month-specific IPC weight will be \\(1/p_a\\). The IPC weights will be computed as the cumulative product of month-specific IPC weights. The final weights to include in outcome analyses will be the product of the sIPT and IPC weights. When estimating \\(p_a\\), individuals who died or were censored by non-informative mechanisms during that month will not be included.\n\n\n\n4.5.3 Missing data\nWe expect the level of missingness to be low, except for uACR.\n\nFor comorbidities and comedication, absence of a record within the lookback period is assumed to be indicative of absence of the condition; missingness will be 0% by definition.\nKnown age, sex and country of origin are required for inclusion; missingness will be 0% by definition.\nWe include biomarkers that are expected to be measured regularly for individuals with T2DM, missingness is expected to be close to 20% for uACR and less than 5% for other biomarkers.\n\nWe will apply multiple imputation to handle missing data.17 Results across imputed datasets will be aggregated using Rubin’s rule. We will use the seed 51389184 when running the imputation model.18\n\n\n4.5.4 Subgroup analyses\nWe will estimate risks and conduct cause specific Cox-regressions in subgroups defined by sex, country of origin and age (&lt;65 years versus 65+ years). These results will be reported as outlined in Figure 5. To assess treatment effect heterogeneity, we will include interaction terms for the exposure and each of these variables in separate cause specific Cox models and report the point estimate and 95% CI.19 If the proportionality assumption is violated, risk ratios will be estimated based on the Aalen-Johansen estimator, and the interaction terms will be estimated by dividing risk ratios across strata. Should this be the case, weights will be re-estimated within each subgroup. We will assess the balance of baseline variables in these subgroups using ASMDs. However, we will not re-evaluate the methods if sufficient balance cannot be achieved. Instead, any imbalance will be reported in the manuscript or supplementary material. Outcome analyses will be conducted regardless of the ASMD-values.\n\n\n4.5.5 Sensitivity analyses\nThese sensitivity analyses are to be applied to the overall ITT analysis only. Result will be reported as outlined in Table 3.\nWe will conduct a complete case analysis, as we are uncertain about the assumptions relating to missingness.\nAs we lack data on BMI, and we assume GLP-1RA is associated with higher levels of BMI, we will repeat the analysis within individuals with an obesity diagnosis.\nWe will restrict the analysis to individuals without a diagnosis of heart failure or CKD, to increase the likelihood that the indication for SGLT2i-initiation was T2DM.\nFor sensitivity analyses restricted to subgroups, we will re-estimate the sIPT weights and we will assess balance by reporting ASMD but potential imbalance will not lead to change in analytic methods.\nWe will rerun outcome analyses truncating weights at 50, to assess robustness (re-estimating the weights will not be necessary if the exact estimates are retained). In relation to this, the number of individuals with truncated weights, along with the median of truncated sIPT weights before truncation will be assessed if more than 20 weights are truncated. For products of sIPT and IPC weights used in PP analyses we will assess the median of the maximum truncated weight for each individual among individuals with at least one truncated weight, assuming at least 20 individuals have truncated weights.\nThe statistical analyses will be conducted using [SAS/R/Stata] version X.X or higher.\n\n\n\n4.6 Compliance with rules from data providers\nNumbers less than 5 will be masked from table 1. As the population will be large and we are not reporting any decimal places, specifying quartiles of continuous variables will not be an issue.\nIf there are less than 5 events in a group that group will not be analyzed with respect to this event.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#quality-control",
    "href": "nuac.html#quality-control",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "5 Quality control",
    "text": "5 Quality control\nTwo statisticians are assigned to this project. They will not do double programming but code written by one will be checked by the other.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#table-shells",
    "href": "nuac.html#table-shells",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "6 Table shells",
    "text": "6 Table shells\n\n\n\n\nTable 1: Percentages should be reported with 1 decimal place, continuous variables should be reported without decimal places.(Naturally, all variables should be listed in a proper SAP.)\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Table shell 1. Baseline characteristics.\n    \n    \n    \n       \n      SGLT2i\n      GLP-1RA\n    \n  \n  \n    N\n\n\n    Age, median (Q1-Q3)\n\n\n    Sex\n\n\n    Calendar year\n\n\n      2016-2017\n\n\n      2018-2019\n\n\n      2020-2021\n\n\n    T2DM characteristics\n\n\n    Duration, years (Q1-Q3)\n\n\n    Glucose lowering drugs\n\n\n    Insulin\n\n\n    Metformin\n\n\n    …\n\n\n    Comorbidities\n\n\n    Heart failure\n\n\n    Chronic kidney disease\n\n\n    …\n\n\n    Comedication\n\n\n    …\n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Table shell 2. Risk and HRs at 5 years.\n    \n    \n    \n      Outcome\n      Exposure\n      Events\n      Risk (95% CI)\n      HR (95% CI)\n    \n  \n  \n    Plague\nSGLT2i\n\n\n(ref)\n    \nGLP-1RA\n\n\n\n    Cholera\nSGLT2i\n\n\n(ref)\n    \nGLP-1RA\n\n\n\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nTable 3\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Table shell 3. Sensitivity and per protocol analyses: Risk and HRs at 5 years.\n    \n    \n    \n      Outcome\n      Sensitivity analysis\n      Exposure\n      Events\n      Risk (95% CI)\n      HR (95% CI)\n    \n  \n  \n    Plague\nComplete case\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWith obesity\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWithout heart failure or CKD\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nPer protocol\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    Cholera\nComplete case\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWith obesity\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nWithout heart failure or CKD\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA\n\n\n\n    \nPer protocol\nSGLT2i\n\n\n(ref)\n    \n\nGLP-1RA",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#figure-shells",
    "href": "nuac.html#figure-shells",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "7 Figure shells",
    "text": "7 Figure shells\nNB. Figure 3, Figure 4, and Figure 5 are simplified mock-ups of what needs to be made for the study. Do not pay particular attention the colors used here.\n\n7.1 Flowchart\nProvide numbers for the flowchart.\n\n\n\n\n\n\n\n\nFigure 2: Flowchart\n\n\n\n\n\n\n\n7.2 Absolute standardized mean differences\nThe ASMD plot will be included in the supplementary materials. Note that there will be ASMDs for each imputed dataset, please include all ASMDs (across imputations) in one figure as seen below. Much less variation between imputations is expected than seen here. Please include and order variables as they appear in Table 1.\n\n\n\n\n\n\n\n\nFigure 3: ASMD plot partial template.\n\n\n\n\n\n\n\n7.3 Risk curves\nThe risk curves should be provided in one figure including 2 panels. Replace “Intervention” and “Control” by “SGLT2i” and “GLP-1RA” in the legend.\n\n\n\n\n\n\nFigure 4: Risk curves template.\n\n\n\n\n\n7.4 Forest plots\nMake one for plague and cholera separately. Add a column to the right, presenting the estimates and 95% CIs for the interaction term from subgroup analyses (as text). Replace “Intervention” and “Control” by “SGLT2i” and “GLP-1RA”, and “Risk ratio” by “Hazard ratio” in the column header. Colors TBD depending on target journal. Note the strata shown here do not align with those to be done in this study.\n\n\n\n\n\n\nFigure 5: Forest plot template.",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#appendix",
    "href": "nuac.html#appendix",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "8 Appendix",
    "text": "8 Appendix\n\n\n\n\nTable 4: Codes to be used in the study. (Naturally, the table should be populated with codes in a proper SAP.)\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Coding table\n    \n    \n    \n      Variable\n      Data source\n      Codes\n      Patient type\n      Diagnosis types\n      Lookback\n      Notes\n    \n  \n  \n    Exposure\nPrescription registry\n\nNA\nNA\nNA\n\n    SGLT2i\n\n\n\n\n\n\n    GLP-1RA\n\n\n\n\n\nExclude brand names Saxenda and Wegovy\n    In-/exclusion\n\n\n\n\n\n\n    T2DM/Glucose lowering drugs\nPrescription registry\n\nNA\nNA\n1 year\n\n    T2DM/HbA1c\nLaboratory registry\n\nNA\nNA\n3 years\nAny HbA1c &gt; … indicates T2DM\n    T2DM/diagnoses\nPatient registry\n\nAll\nPrimary, secondary\n10 years\n\n    Recent plague or Cholera\nPatient registry\n\nAll\nPrimary, secondary\n90 days\n\n    …\n\n\n\n\n\n\n    Outcomes\nPatient registry\n\nInpatient\nPrimary\n\nOnly at a department of infectious diseases\n    Plague\n\n\n\n\n\n\n    Cholera\n\n\n\n\n\n\n    Comorbidities\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Comedication\n\n\n\n\n\n\n    …\n\n\n\n\n\n\n    Biomarkers\n\n\n\n\n\n\n    …",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  },
  {
    "objectID": "nuac.html#footnotes",
    "href": "nuac.html#footnotes",
    "title": "SGLT2i vs GLP-1RA and the risk of Plague and Cholera",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn an actual SAP you should provide references throughout the document as is relevant. In this example none will be provided.↩︎\nThis is likely a stupid study to conduct in Denmark. That is intentional.↩︎\nA data dictionary or other forms of documentation can be essential if the data sources are unknown to the statistical programmer, or if data can be made publicly available so that others can reproduce your results.↩︎\nNote how the dates are not written here - they are defined above and we seek to avoid repetitions.↩︎\nThe colors and notation in Figure 1 are not the ones typically used. No particular thought has been put into the current design.↩︎\nAlternatives include stacking, i.e., using the sum of \\(\\text{volapk}\\) (taking into account grace periods) of overlapping treatment episodes to estimate when an individual is no longer exposed; or, assuming the second prescription will sometimes run out before the first, use the latter date one of the two treatment episodes end (e.g., the first one covers 90 days, the second is filled 47 days later and covers 30 days, what is done in this study is to use the end of the second prescription - day 77 - even though the first prescription runs until day 90) and then add a grace period from this date.↩︎\nHere should be an elaboration on how to use the different variables to determine the country of origin: there are variables for both nationality and country of birth, potentially data on ((great) grand) parents’ could be used as well.↩︎\nThe term inpatient is not defined in the Danish National Patient Registry v3, therefore, there ought to be a definition of what is meant by it here. This is intentionally excluded from this example.↩︎\nIt would probably be a good idea to consider doing something to assess the quality of the data here. What is the observed range, and is is within the plausible values of these biomarkers? What is the range these biomarkers can be measured at? Are there many with an observations at the limits? If so, is it a concern; do you think the value has been truncated, and should you do something about that?↩︎\nAn alternative could be to compute the standard deviation based on an appropriate transformation of the results, and using this standard deviation as an estimate of the standard error.↩︎\nSpecifying the seed in the SAP may be over the top, particularly if your sample and number of outcomes are large, as you are unlikely to have time to fish for a pleasing seed. However, specifying that a seed should be used for reproducibility is relevant, and seeds need to be arbitrary↩︎\nConsider specifying.↩︎\nTo prevent p-hacking or HARKing, consider deciding a priori on certain intermediate milestones where analyses will be paused, and findings/results so far will be discussed in the group. In this way you can prevent yourself from changing the analyses after seeing the primary results, when it could have been done at an earlier stage.↩︎\nOften this is what you would do anyway when using PS-weighting or -matching. Cox-regression is primarily mentioned to point out that PS-methods can be combined with a multivariable outcome-model.↩︎\nThis would have to be revised in an actual SAP, and the exact methods specified should not be taken as gospel. E.g., stabilization of IPC weights should be considered. What is included here is simply a hint as to what needs to be considered before estimating IPC weights.↩︎\nTechnically what is used is not the probability of being censored but the probability of remaining uncensored. The term IPC is somewhat misleading.↩︎\nMore details should be provided - which imputation method to use, which variables to include, how many imputations, etc.↩︎\nSee comment regarding seed for bootstrapping.↩︎\nOr however you want to do that. If you simply go for eyeballing you might want to rewrite/shorten this section a bit.↩︎",
    "crumbs": [
      "Pharmacoepidemiology",
      "SGLT2i vs GLP-1RA and the risk of Plague and Cholera"
    ]
  }
]